{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Article / Sentence RNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1PMbnkhw1q4v","executionInfo":{"status":"ok","timestamp":1621344535321,"user_tz":240,"elapsed":20025,"user":{"displayName":"Jack Schooley","photoUrl":"","userId":"14946235557075291110"}},"outputId":"63d8eaab-f4d6-4ef3-dc3e-95b5f091f659"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UCCxrpJk-Bx7"},"source":["Strip Data"]},{"cell_type":"code","metadata":{"id":"3Qm2gNOp19d0"},"source":["import glob\n","import pandas as pd\n","\n","train_df = pd.DataFrame(columns = [\"Author\", \"Article\"], dtype = str)\n","for i, filename in enumerate(glob.iglob(\"/content/gdrive/My Drive/C50/C50train/*/*.txt\")):\n","  if i % 100 == 0:\n","    print(i)\n","  author = filename.split(\"/\")[6]\n","  with open(filename) as file:\n","    text = file.readlines()\n","  train_df.loc[i] = [author, text]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQf1kGwC6rFy"},"source":["test_df = pd.DataFrame(columns = [\"Author\", \"Article\"], dtype = str)\n","for i, filename in enumerate(glob.iglob(\"/content/gdrive/My Drive/C50/C50test/*/*.txt\")):\n","  if i % 100 == 0:\n","    print(i)\n","  author = filename.split(\"/\")[6]\n","  with open(filename) as file:\n","    text = file.readlines()\n","  test_df.loc[i] = [author, text]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8C3kGPia99Qi"},"source":["import os\n","\n","os.chdir(\"/content/gdrive/My Drive/C50/\")\n","train_df.to_csv(\"train_df.csv\", index = False)\n","test_df.to_csv(\"test_df.csv\", index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kC6IBjyB-Fu9"},"source":["Preprocessing"]},{"cell_type":"code","metadata":{"id":"dKZuvESp44t-"},"source":["import re\n","\n","def parse_sentence(sentence):\n","  words = []\n","  for word in sentence.split():\n","    parsed_word = re.sub(\"\\W\", \"\", word)\n","    parsed_word = parsed_word.lower()\n","    if parsed_word:\n","      words.append(parsed_word)\n","  return words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TkiZccNIDs5g"},"source":["def parse_article(text):\n","  sentences = []\n","  sentence_lengths = []\n","  article = text.split(\"\\\\n\")\n","  for sentence in article:\n","    parsed_sentence = parse_sentence(sentence)\n","    if parsed_sentence:\n","      sentence_length = len(parsed_sentence)\n","      sentences.append(parsed_sentence)\n","      sentence_lengths.append(sentence_length)\n","  return sentences, sentence_lengths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4dinw1foFaAY"},"source":["import numpy as np\n","\n","glove_embeddings = {}\n","with open(\"/content/gdrive/My Drive/C50/glove.6B.50d.txt\") as embedding_file:\n","  for index, line in enumerate(embedding_file):\n","    embedding = line.split()\n","    word = embedding[0]\n","    vector = np.asarray(embedding[1:], \"float32\")\n","    glove_embeddings[word] = vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W7jIf6lvNdUM"},"source":["import torch\n","\n","word_to_index = {}\n","\n","def preprocess(article, return_lengths = False):\n","  sentences, sentence_lengths = parse_article(article)\n","  if return_lengths:\n","    return sentence_lengths\n","  max_sentence_length = max(sentence_lengths)\n","  article_tensor = torch.zeros(len(sentences), max_sentence_length, dtype = torch.int64)\n","  for i, sentence in enumerate(sentences):\n","    sentence_length = sentence_lengths[i]\n","    for j, word in enumerate(sentence):\n","      index = word_to_index.get(word)\n","      if index is None:\n","        if word in glove_embeddings.keys():\n","          index = len(word_to_index) + 1\n","          word_to_index[word] = index\n","        else:\n","          index = 0\n","      article_tensor[i, j] = index\n","    for j in range(sentence_length, max_sentence_length):\n","      article_tensor[i, j] = 0\n","  return article_tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pTy2XlCdlqri"},"source":["import os\n","import pandas as pd\n","\n","os.chdir(\"/content/gdrive/My Drive/C50/\")\n","train_df = pd.read_csv(\"train_df.csv\")\n","test_df = pd.read_csv(\"test_df.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QP-ToAyzpVTP"},"source":["authors = train_df[\"Author\"].unique()\n","author_dict = {author:index for (index, author) in enumerate(authors)}\n","\n","for author in authors:\n","  author_df = test_df[test_df[\"Author\"] == author]\n","  author_train = author_df.sample(frac = 0.8, random_state = 87)\n","  train_df = train_df.append(author_train, True)\n","  test_df.drop(index = author_train.index, inplace = True)\n","test_df.index = range(test_df.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJI6qYEnkkvq"},"source":["train_df[\"Tensors\"] = train_df[\"Article\"].apply(preprocess)\n","train_df[\"Sentence Lengths\"] = train_df[\"Article\"].apply(preprocess, return_lengths = True)\n","test_df[\"Tensors\"] = test_df[\"Article\"].apply(preprocess)\n","test_df[\"Sentence Lengths\"] = test_df[\"Article\"].apply(preprocess, return_lengths = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"caoSiZYJLmWW"},"source":["initial_embeddings = torch.zeros((len(word_to_index) + 1, 50), dtype = torch.float32)\n","for word in word_to_index.keys():\n","  index = word_to_index[word]\n","  embedding = glove_embeddings[word]\n","  initial_embeddings[index, :] = torch.from_numpy(embedding)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWs4OdrQ93lg"},"source":["Sentence Level GRU"]},{"cell_type":"code","metadata":{"id":"3kym9DwXa4d7"},"source":["from torch import nn\n","\n","class SentenceGRU(nn.Module):\n","\n","  def __init__(self, hidden_size, initial_embeddings):\n","    super().__init__()\n","    self.embedding_dim = initial_embeddings.size(1)\n","    self.embedding = nn.Embedding.from_pretrained(initial_embeddings, False, 0)\n","    self.gru = nn.GRU(self.embedding_dim, hidden_size, batch_first = True)\n","    self.linear = nn.Linear(hidden_size, 50)\n","\n","  def forward(self, article_tensor, sentence_lengths):\n","    embeddings = self.embedding(article_tensor)\n","    gru_inputs = nn.utils.rnn.pack_padded_sequence(embeddings, sentence_lengths, True, False)\n","    gru_outputs, hidden_state = self.gru(gru_inputs)\n","    predictions = self.linear(hidden_state[0, :, :])\n","    return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aYlkrWQZbJXO"},"source":["from torch import optim\n","\n","hidden_size = 300\n","learning_rate = 0.02\n","num_epochs = 10\n","\n","model = SentenceGRU(hidden_size, initial_embeddings).cuda()\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ccSv7HsmfhJd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621313607527,"user_tz":240,"elapsed":221590,"user":{"displayName":"Jack Schooley","photoUrl":"","userId":"14946235557075291110"}},"outputId":"da8f03cf-f0ad-4fc7-e0bd-0dbd698f8336"},"source":["model.train()\n","batches = train_df.shape[0]\n","train_loop_df = train_df.sample(frac = 1, random_state = 2).reset_index()\n","for epoch in range(num_epochs):\n","  print(\"Epoch \" + str(epoch))\n","  for batch in range(batches):\n","    tensor = train_loop_df.loc[batch, \"Tensors\"]\n","    sentence_lengths = train_loop_df.loc[batch, \"Sentence Lengths\"]\n","    \n","    inputs = tensor.cuda()\n","    outputs = model(inputs, sentence_lengths)\n","\n","    batch_size = tensor.size(0)\n","    author = train_loop_df.loc[batch, \"Author\"]\n","    author_tensor = torch.LongTensor([author_dict[author] for batch in range(batch_size)]).cuda()\n","\n","    loss = loss_fn(outputs, author_tensor)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if batch % 100 == 0:\n","      print(\"Batch \" + str(batch) + \", loss is \" + str(loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0\n","Batch 0, loss is 4.066736698150635\n","Batch 100, loss is 4.0660529136657715\n","Batch 200, loss is 4.077706336975098\n","Batch 300, loss is 3.672333240509033\n","Batch 400, loss is 3.476971387863159\n","Batch 500, loss is 3.8298075199127197\n","Batch 600, loss is 3.8775546550750732\n","Batch 700, loss is 3.6111555099487305\n","Batch 800, loss is 3.7829079627990723\n","Batch 900, loss is 3.6850340366363525\n","Batch 1000, loss is 4.061131954193115\n","Batch 1100, loss is 3.8233444690704346\n","Batch 1200, loss is 3.7961812019348145\n","Batch 1300, loss is 4.101495265960693\n","Batch 1400, loss is 3.7292892932891846\n","Batch 1500, loss is 3.773738384246826\n","Batch 1600, loss is 3.635838270187378\n","Batch 1700, loss is 3.7024927139282227\n","Batch 1800, loss is 3.5741124153137207\n","Batch 1900, loss is 3.458127498626709\n","Batch 2000, loss is 3.308328151702881\n","Batch 2100, loss is 3.692006826400757\n","Batch 2200, loss is 3.437983989715576\n","Batch 2300, loss is 3.9135892391204834\n","Batch 2400, loss is 4.361871242523193\n","Batch 2500, loss is 3.5496573448181152\n","Batch 2600, loss is 3.7289459705352783\n","Batch 2700, loss is 3.771070718765259\n","Batch 2800, loss is 3.5940518379211426\n","Batch 2900, loss is 3.4770846366882324\n","Batch 3000, loss is 4.073652267456055\n","Batch 3100, loss is 3.3035261631011963\n","Batch 3200, loss is 2.5303971767425537\n","Batch 3300, loss is 2.9565112590789795\n","Batch 3400, loss is 3.2728641033172607\n","Batch 3500, loss is 3.5113584995269775\n","Batch 3600, loss is 2.96136474609375\n","Batch 3700, loss is 3.382260322570801\n","Batch 3800, loss is 3.0555553436279297\n","Batch 3900, loss is 1.3321139812469482\n","Batch 4000, loss is 3.633927583694458\n","Batch 4100, loss is 3.336291790008545\n","Batch 4200, loss is 3.8343982696533203\n","Batch 4300, loss is 3.5042216777801514\n","Batch 4400, loss is 2.8240716457366943\n","Epoch 1\n","Batch 0, loss is 2.3588693141937256\n","Batch 100, loss is 3.0520973205566406\n","Batch 200, loss is 2.4285688400268555\n","Batch 300, loss is 3.5428173542022705\n","Batch 400, loss is 2.4430794715881348\n","Batch 500, loss is 3.218789577484131\n","Batch 600, loss is 4.032914161682129\n","Batch 700, loss is 2.98966646194458\n","Batch 800, loss is 3.2976136207580566\n","Batch 900, loss is 2.2874834537506104\n","Batch 1000, loss is 3.5632779598236084\n","Batch 1100, loss is 3.512918710708618\n","Batch 1200, loss is 2.520352840423584\n","Batch 1300, loss is 3.6402506828308105\n","Batch 1400, loss is 3.0164968967437744\n","Batch 1500, loss is 2.804999351501465\n","Batch 1600, loss is 3.149336338043213\n","Batch 1700, loss is 2.719698905944824\n","Batch 1800, loss is 3.3534040451049805\n","Batch 1900, loss is 3.119673252105713\n","Batch 2000, loss is 3.2952451705932617\n","Batch 2100, loss is 3.5281667709350586\n","Batch 2200, loss is 1.8930041790008545\n","Batch 2300, loss is 3.5543761253356934\n","Batch 2400, loss is 5.1239013671875\n","Batch 2500, loss is 2.66127347946167\n","Batch 2600, loss is 3.8567159175872803\n","Batch 2700, loss is 3.6741209030151367\n","Batch 2800, loss is 3.4933249950408936\n","Batch 2900, loss is 4.54239559173584\n","Batch 3000, loss is 3.543076515197754\n","Batch 3100, loss is 2.5383920669555664\n","Batch 3200, loss is 2.274401903152466\n","Batch 3300, loss is 3.337271213531494\n","Batch 3400, loss is 3.0455574989318848\n","Batch 3500, loss is 3.0274546146392822\n","Batch 3600, loss is 2.6941468715667725\n","Batch 3700, loss is 2.3160178661346436\n","Batch 3800, loss is 3.2064731121063232\n","Batch 3900, loss is 1.175065279006958\n","Batch 4000, loss is 2.617197275161743\n","Batch 4100, loss is 3.224473237991333\n","Batch 4200, loss is 4.204257965087891\n","Batch 4300, loss is 3.2376511096954346\n","Batch 4400, loss is 2.062159299850464\n","Epoch 2\n","Batch 0, loss is 1.4591726064682007\n","Batch 100, loss is 2.9102413654327393\n","Batch 200, loss is 1.5911535024642944\n","Batch 300, loss is 3.369255304336548\n","Batch 400, loss is 1.6506401300430298\n","Batch 500, loss is 2.602785110473633\n","Batch 600, loss is 4.250524520874023\n","Batch 700, loss is 2.710773468017578\n","Batch 800, loss is 2.620227813720703\n","Batch 900, loss is 1.8410266637802124\n","Batch 1000, loss is 3.2809009552001953\n","Batch 1100, loss is 2.88865065574646\n","Batch 1200, loss is 1.9009857177734375\n","Batch 1300, loss is 3.2677931785583496\n","Batch 1400, loss is 2.3900444507598877\n","Batch 1500, loss is 2.509669780731201\n","Batch 1600, loss is 2.8896336555480957\n","Batch 1700, loss is 1.511320948600769\n","Batch 1800, loss is 2.4491169452667236\n","Batch 1900, loss is 2.5033671855926514\n","Batch 2000, loss is 2.6783015727996826\n","Batch 2100, loss is 3.1397078037261963\n","Batch 2200, loss is 1.1234992742538452\n","Batch 2300, loss is 3.020500421524048\n","Batch 2400, loss is 4.947279453277588\n","Batch 2500, loss is 1.9824551343917847\n","Batch 2600, loss is 3.7041561603546143\n","Batch 2700, loss is 2.9439380168914795\n","Batch 2800, loss is 2.601818799972534\n","Batch 2900, loss is 3.914292097091675\n","Batch 3000, loss is 3.1139848232269287\n","Batch 3100, loss is 1.9488484859466553\n","Batch 3200, loss is 1.3902627229690552\n","Batch 3300, loss is 3.37247896194458\n","Batch 3400, loss is 3.0470728874206543\n","Batch 3500, loss is 2.875336170196533\n","Batch 3600, loss is 2.691901445388794\n","Batch 3700, loss is 1.6750319004058838\n","Batch 3800, loss is 2.5959320068359375\n","Batch 3900, loss is 0.9071106910705566\n","Batch 4000, loss is 2.1940245628356934\n","Batch 4100, loss is 2.750866651535034\n","Batch 4200, loss is 4.349367141723633\n","Batch 4300, loss is 3.1778621673583984\n","Batch 4400, loss is 1.6724284887313843\n","Epoch 3\n","Batch 0, loss is 1.6505579948425293\n","Batch 100, loss is 2.9759907722473145\n","Batch 200, loss is 1.6514453887939453\n","Batch 300, loss is 1.7567285299301147\n","Batch 400, loss is 1.245140552520752\n","Batch 500, loss is 2.1467440128326416\n","Batch 600, loss is 3.894298791885376\n","Batch 700, loss is 2.4638495445251465\n","Batch 800, loss is 1.795192003250122\n","Batch 900, loss is 1.407065987586975\n","Batch 1000, loss is 2.6121883392333984\n","Batch 1100, loss is 2.387983560562134\n","Batch 1200, loss is 1.8347405195236206\n","Batch 1300, loss is 2.9541938304901123\n","Batch 1400, loss is 1.5991191864013672\n","Batch 1500, loss is 1.7661992311477661\n","Batch 1600, loss is 2.591712474822998\n","Batch 1700, loss is 0.708045482635498\n","Batch 1800, loss is 1.7376682758331299\n","Batch 1900, loss is 1.846890926361084\n","Batch 2000, loss is 2.5089423656463623\n","Batch 2100, loss is 2.4936442375183105\n","Batch 2200, loss is 0.7632070779800415\n","Batch 2300, loss is 2.6557514667510986\n","Batch 2400, loss is 3.878821849822998\n","Batch 2500, loss is 1.919569492340088\n","Batch 2600, loss is 3.239435911178589\n","Batch 2700, loss is 2.7265114784240723\n","Batch 2800, loss is 2.2289881706237793\n","Batch 2900, loss is 3.3573760986328125\n","Batch 3000, loss is 2.6001839637756348\n","Batch 3100, loss is 1.8982043266296387\n","Batch 3200, loss is 0.9305147528648376\n","Batch 3300, loss is 3.155412197113037\n","Batch 3400, loss is 2.871201992034912\n","Batch 3500, loss is 3.0651490688323975\n","Batch 3600, loss is 2.474970579147339\n","Batch 3700, loss is 1.3077902793884277\n","Batch 3800, loss is 2.27533221244812\n","Batch 3900, loss is 0.8032987713813782\n","Batch 4000, loss is 2.029391288757324\n","Batch 4100, loss is 2.4222264289855957\n","Batch 4200, loss is 4.621750831604004\n","Batch 4300, loss is 2.7238614559173584\n","Batch 4400, loss is 1.5545958280563354\n","Epoch 4\n","Batch 0, loss is 1.716842532157898\n","Batch 100, loss is 2.912885904312134\n","Batch 200, loss is 1.4357428550720215\n","Batch 300, loss is 1.6069068908691406\n","Batch 400, loss is 1.165737509727478\n","Batch 500, loss is 1.8866615295410156\n","Batch 600, loss is 3.760146379470825\n","Batch 700, loss is 2.402383327484131\n","Batch 800, loss is 1.5774004459381104\n","Batch 900, loss is 1.163718819618225\n","Batch 1000, loss is 2.3135311603546143\n","Batch 1100, loss is 2.327561140060425\n","Batch 1200, loss is 1.4464884996414185\n","Batch 1300, loss is 2.618633508682251\n","Batch 1400, loss is 1.1859480142593384\n","Batch 1500, loss is 1.459776520729065\n","Batch 1600, loss is 2.3620944023132324\n","Batch 1700, loss is 0.4700915813446045\n","Batch 1800, loss is 1.586586594581604\n","Batch 1900, loss is 1.5482147932052612\n","Batch 2000, loss is 2.3968629837036133\n","Batch 2100, loss is 2.018944025039673\n","Batch 2200, loss is 0.7044141292572021\n","Batch 2300, loss is 2.417313575744629\n","Batch 2400, loss is 3.4314987659454346\n","Batch 2500, loss is 1.7935442924499512\n","Batch 2600, loss is 2.8681142330169678\n","Batch 2700, loss is 2.517392635345459\n","Batch 2800, loss is 2.1503872871398926\n","Batch 2900, loss is 3.0951220989227295\n","Batch 3000, loss is 2.4705119132995605\n","Batch 3100, loss is 1.8324087858200073\n","Batch 3200, loss is 0.7967512607574463\n","Batch 3300, loss is 2.9737443923950195\n","Batch 3400, loss is 2.7639107704162598\n","Batch 3500, loss is 3.0803818702697754\n","Batch 3600, loss is 2.278489351272583\n","Batch 3700, loss is 1.0756498575210571\n","Batch 3800, loss is 2.1168577671051025\n","Batch 3900, loss is 0.8163642883300781\n","Batch 4000, loss is 1.9353951215744019\n","Batch 4100, loss is 2.232842445373535\n","Batch 4200, loss is 4.657065391540527\n","Batch 4300, loss is 2.388470411300659\n","Batch 4400, loss is 1.3940479755401611\n","Epoch 5\n","Batch 0, loss is 1.7459989786148071\n","Batch 100, loss is 2.846649646759033\n","Batch 200, loss is 1.3481501340866089\n","Batch 300, loss is 1.6454007625579834\n","Batch 400, loss is 1.0262749195098877\n","Batch 500, loss is 1.6134169101715088\n","Batch 600, loss is 3.490495443344116\n","Batch 700, loss is 2.409014940261841\n","Batch 800, loss is 1.4833685159683228\n","Batch 900, loss is 1.0152958631515503\n","Batch 1000, loss is 2.1689488887786865\n","Batch 1100, loss is 2.229344367980957\n","Batch 1200, loss is 1.0435134172439575\n","Batch 1300, loss is 2.3285911083221436\n","Batch 1400, loss is 0.963510274887085\n","Batch 1500, loss is 1.304317831993103\n","Batch 1600, loss is 2.1741254329681396\n","Batch 1700, loss is 0.400568425655365\n","Batch 1800, loss is 1.5465631484985352\n","Batch 1900, loss is 1.3482998609542847\n","Batch 2000, loss is 2.237755298614502\n","Batch 2100, loss is 1.6216521263122559\n","Batch 2200, loss is 0.6339995265007019\n","Batch 2300, loss is 2.1588072776794434\n","Batch 2400, loss is 3.315814971923828\n","Batch 2500, loss is 1.6833633184432983\n","Batch 2600, loss is 2.5606508255004883\n","Batch 2700, loss is 2.3069984912872314\n","Batch 2800, loss is 2.0392026901245117\n","Batch 2900, loss is 2.9479613304138184\n","Batch 3000, loss is 2.464935779571533\n","Batch 3100, loss is 1.7287025451660156\n","Batch 3200, loss is 0.7074703574180603\n","Batch 3300, loss is 2.853919506072998\n","Batch 3400, loss is 2.6631648540496826\n","Batch 3500, loss is 2.8916056156158447\n","Batch 3600, loss is 2.180280923843384\n","Batch 3700, loss is 0.9462122321128845\n","Batch 3800, loss is 2.0355348587036133\n","Batch 3900, loss is 0.8505991101264954\n","Batch 4000, loss is 1.8552117347717285\n","Batch 4100, loss is 2.1395785808563232\n","Batch 4200, loss is 4.483580112457275\n","Batch 4300, loss is 2.0643482208251953\n","Batch 4400, loss is 1.2740156650543213\n","Epoch 6\n","Batch 0, loss is 1.7678099870681763\n","Batch 100, loss is 2.7849040031433105\n","Batch 200, loss is 1.3035675287246704\n","Batch 300, loss is 1.6492732763290405\n","Batch 400, loss is 0.8969919085502625\n","Batch 500, loss is 1.3816245794296265\n","Batch 600, loss is 3.238222360610962\n","Batch 700, loss is 2.4370739459991455\n","Batch 800, loss is 1.3733088970184326\n","Batch 900, loss is 0.8820416331291199\n","Batch 1000, loss is 2.118661642074585\n","Batch 1100, loss is 2.0652756690979004\n","Batch 1200, loss is 0.7946141362190247\n","Batch 1300, loss is 2.1352386474609375\n","Batch 1400, loss is 0.8241212368011475\n","Batch 1500, loss is 1.1991281509399414\n","Batch 1600, loss is 2.0391829013824463\n","Batch 1700, loss is 0.37515318393707275\n","Batch 1800, loss is 1.5214077234268188\n","Batch 1900, loss is 1.1915276050567627\n","Batch 2000, loss is 2.0958938598632812\n","Batch 2100, loss is 1.3575495481491089\n","Batch 2200, loss is 0.573341965675354\n","Batch 2300, loss is 1.98540461063385\n","Batch 2400, loss is 3.291106700897217\n","Batch 2500, loss is 1.5954796075820923\n","Batch 2600, loss is 2.285888910293579\n","Batch 2700, loss is 2.14581561088562\n","Batch 2800, loss is 1.9698656797409058\n","Batch 2900, loss is 2.852362871170044\n","Batch 3000, loss is 2.4551355838775635\n","Batch 3100, loss is 1.6993992328643799\n","Batch 3200, loss is 0.6351994872093201\n","Batch 3300, loss is 2.765383243560791\n","Batch 3400, loss is 2.570439338684082\n","Batch 3500, loss is 2.71809983253479\n","Batch 3600, loss is 2.0993239879608154\n","Batch 3700, loss is 0.8958603143692017\n","Batch 3800, loss is 2.0007989406585693\n","Batch 3900, loss is 0.8834692239761353\n","Batch 4000, loss is 1.790927529335022\n","Batch 4100, loss is 2.105541706085205\n","Batch 4200, loss is 4.25036096572876\n","Batch 4300, loss is 1.8072503805160522\n","Batch 4400, loss is 1.1839287281036377\n","Epoch 7\n","Batch 0, loss is 1.7715892791748047\n","Batch 100, loss is 2.7258565425872803\n","Batch 200, loss is 1.261481523513794\n","Batch 300, loss is 1.6432316303253174\n","Batch 400, loss is 0.7765775918960571\n","Batch 500, loss is 1.219001293182373\n","Batch 600, loss is 3.0533900260925293\n","Batch 700, loss is 2.447836399078369\n","Batch 800, loss is 1.2571041584014893\n","Batch 900, loss is 0.7624013423919678\n","Batch 1000, loss is 2.0798747539520264\n","Batch 1100, loss is 1.9031708240509033\n","Batch 1200, loss is 0.6428437829017639\n","Batch 1300, loss is 2.0162487030029297\n","Batch 1400, loss is 0.7273113131523132\n","Batch 1500, loss is 1.1314265727996826\n","Batch 1600, loss is 1.9623523950576782\n","Batch 1700, loss is 0.36814090609550476\n","Batch 1800, loss is 1.4970417022705078\n","Batch 1900, loss is 1.0704385042190552\n","Batch 2000, loss is 1.981044054031372\n","Batch 2100, loss is 1.1819379329681396\n","Batch 2200, loss is 0.5293145179748535\n","Batch 2300, loss is 1.879000186920166\n","Batch 2400, loss is 3.281002998352051\n","Batch 2500, loss is 1.5252044200897217\n","Batch 2600, loss is 2.0643575191497803\n","Batch 2700, loss is 2.0786728858947754\n","Batch 2800, loss is 1.9225062131881714\n","Batch 2900, loss is 2.7701573371887207\n","Batch 3000, loss is 2.4496397972106934\n","Batch 3100, loss is 1.7184332609176636\n","Batch 3200, loss is 0.5774403214454651\n","Batch 3300, loss is 2.689852714538574\n","Batch 3400, loss is 2.504586935043335\n","Batch 3500, loss is 2.573446035385132\n","Batch 3600, loss is 2.019052505493164\n","Batch 3700, loss is 0.8749316930770874\n","Batch 3800, loss is 1.9833711385726929\n","Batch 3900, loss is 0.9158061146736145\n","Batch 4000, loss is 1.7313402891159058\n","Batch 4100, loss is 2.0992777347564697\n","Batch 4200, loss is 4.0372090339660645\n","Batch 4300, loss is 1.6241508722305298\n","Batch 4400, loss is 1.1198952198028564\n","Epoch 8\n","Batch 0, loss is 1.7555733919143677\n","Batch 100, loss is 2.6722984313964844\n","Batch 200, loss is 1.2287137508392334\n","Batch 300, loss is 1.6338223218917847\n","Batch 400, loss is 0.6614664196968079\n","Batch 500, loss is 1.112939476966858\n","Batch 600, loss is 2.9202983379364014\n","Batch 700, loss is 2.4276440143585205\n","Batch 800, loss is 1.1375813484191895\n","Batch 900, loss is 0.6621152758598328\n","Batch 1000, loss is 2.026034116744995\n","Batch 1100, loss is 1.777716875076294\n","Batch 1200, loss is 0.5467756986618042\n","Batch 1300, loss is 1.9296441078186035\n","Batch 1400, loss is 0.659949779510498\n","Batch 1500, loss is 1.0800648927688599\n","Batch 1600, loss is 1.9195932149887085\n","Batch 1700, loss is 0.3549323081970215\n","Batch 1800, loss is 1.475311517715454\n","Batch 1900, loss is 0.9806873202323914\n","Batch 2000, loss is 1.893405795097351\n","Batch 2100, loss is 1.0505709648132324\n","Batch 2200, loss is 0.49980661273002625\n","Batch 2300, loss is 1.7931376695632935\n","Batch 2400, loss is 3.2636466026306152\n","Batch 2500, loss is 1.468380331993103\n","Batch 2600, loss is 1.8955867290496826\n","Batch 2700, loss is 2.0854547023773193\n","Batch 2800, loss is 1.8812055587768555\n","Batch 2900, loss is 2.680011749267578\n","Batch 3000, loss is 2.4545159339904785\n","Batch 3100, loss is 1.73940110206604\n","Batch 3200, loss is 0.5276442766189575\n","Batch 3300, loss is 2.6212446689605713\n","Batch 3400, loss is 2.4596405029296875\n","Batch 3500, loss is 2.444932699203491\n","Batch 3600, loss is 1.9442706108093262\n","Batch 3700, loss is 0.8637138605117798\n","Batch 3800, loss is 1.9683328866958618\n","Batch 3900, loss is 0.9466058015823364\n","Batch 4000, loss is 1.6712979078292847\n","Batch 4100, loss is 2.1009328365325928\n","Batch 4200, loss is 3.854921340942383\n","Batch 4300, loss is 1.4909921884536743\n","Batch 4400, loss is 1.0778065919876099\n","Epoch 9\n","Batch 0, loss is 1.725572109222412\n","Batch 100, loss is 2.6220946311950684\n","Batch 200, loss is 1.205910325050354\n","Batch 300, loss is 1.6204062700271606\n","Batch 400, loss is 0.5631592869758606\n","Batch 500, loss is 1.0401476621627808\n","Batch 600, loss is 2.8049752712249756\n","Batch 700, loss is 2.3821990489959717\n","Batch 800, loss is 1.0226596593856812\n","Batch 900, loss is 0.5871859192848206\n","Batch 1000, loss is 1.959309697151184\n","Batch 1100, loss is 1.6894398927688599\n","Batch 1200, loss is 0.4825386703014374\n","Batch 1300, loss is 1.849440097808838\n","Batch 1400, loss is 0.611497163772583\n","Batch 1500, loss is 1.0402498245239258\n","Batch 1600, loss is 1.8931167125701904\n","Batch 1700, loss is 0.3377181887626648\n","Batch 1800, loss is 1.4559602737426758\n","Batch 1900, loss is 0.9103097319602966\n","Batch 2000, loss is 1.8317798376083374\n","Batch 2100, loss is 0.9414260387420654\n","Batch 2200, loss is 0.4822653532028198\n","Batch 2300, loss is 1.7082637548446655\n","Batch 2400, loss is 3.2209935188293457\n","Batch 2500, loss is 1.4197032451629639\n","Batch 2600, loss is 1.7644935846328735\n","Batch 2700, loss is 2.1269257068634033\n","Batch 2800, loss is 1.839738368988037\n","Batch 2900, loss is 2.579293966293335\n","Batch 3000, loss is 2.460500478744507\n","Batch 3100, loss is 1.7496857643127441\n","Batch 3200, loss is 0.4808965027332306\n","Batch 3300, loss is 2.5603692531585693\n","Batch 3400, loss is 2.4237115383148193\n","Batch 3500, loss is 2.325542449951172\n","Batch 3600, loss is 1.8819860219955444\n","Batch 3700, loss is 0.8550373911857605\n","Batch 3800, loss is 1.9563807249069214\n","Batch 3900, loss is 0.9752967953681946\n","Batch 4000, loss is 1.6116443872451782\n","Batch 4100, loss is 2.0941238403320312\n","Batch 4200, loss is 3.6962201595306396\n","Batch 4300, loss is 1.3911715745925903\n","Batch 4400, loss is 1.0511854887008667\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZFEnBHUvfNZ-"},"source":["model.eval()\n","batches = test_df.shape[0]\n","final_labels = None\n","final_probs = None\n","for batch in range(batches):\n","  tensor = test_df.loc[batch, \"Tensors\"]\n","  sentence_lengths = test_df.loc[batch, \"Sentence Lengths\"]\n","\n","  inputs = tensor.cuda()\n","  \n","  with torch.no_grad():\n","    outputs = model(inputs, sentence_lengths)\n","  prob_tensor = nn.functional.softmax(outputs, 1)\n","\n","  batch_size = tensor.shape[0]\n","  author = test_df.loc[batch, \"Author\"]\n","  label_tensor = torch.LongTensor([author_dict[author] for batch in range(batch_size)])\n","\n","  if final_labels is None:\n","    final_labels = label_tensor\n","  else:\n","    final_labels = torch.cat([final_labels, label_tensor])\n","  \n","  if final_probs is None:\n","    final_probs = prob_tensor\n","  else:\n","    final_probs = torch.cat([final_probs, prob_tensor])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3hGotI8gBqt7","executionInfo":{"status":"ok","timestamp":1621313619370,"user_tz":240,"elapsed":711,"user":{"displayName":"Jack Schooley","photoUrl":"","userId":"14946235557075291110"}},"outputId":"78ac5489-566c-46de-f5ff-7ab723d622d7"},"source":["from sklearn import metrics\n","\n","labels = final_labels.numpy()\n","probs = final_probs.cpu().numpy()\n","preds = np.argmax(probs, 1)\n","\n","metrics.accuracy_score(labels, preds)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.39871491376394996"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"Mqxdl3JIdbd5"},"source":["Article Level GRU"]},{"cell_type":"code","metadata":{"id":"zKYONyH-rc13"},"source":["def pad_article_tensor(tensor, second_sequence_length):\n","  if tensor.size(1) > second_sequence_length:\n","    return tensor[:, :second_sequence_length]\n","  article_length = tensor.size(0)\n","  while tensor.size(1) < second_sequence_length:\n","    magic_word_tensor = torch.LongTensor([0 for i in range(article_length)])\n","    mw_unsqueezed = magic_word_tensor.unsqueeze(1)\n","    tensor = torch.cat([tensor, mw_unsqueezed], 1)\n","  return tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cPVQt2Fto-y"},"source":["def pad_batch(tensor, article_lengths):\n","  max_article_length = max(article_lengths)\n","  while tensor.size(0) < max_article_length:\n","    magic_word_tensor = torch.LongTensor([0 for i in range(tensor.size(1))])\n","    mw_unsqueezed = magic_word_tensor.unsqueeze(0)\n","    tensor = torch.cat([tensor, mw_unsqueezed], 0)\n","  return tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ukTo-mAMdele"},"source":["from torch import nn\n","\n","class ArticleGRU(nn.Module):\n","\n","  def __init__(self, sequence_length, hidden_size, initial_embeddings):\n","    super().__init__()\n","    self.embedding_dim = initial_embeddings.size(1)\n","    self.embedding = nn.Embedding.from_pretrained(initial_embeddings, False, 0)\n","    self.pool = nn.AvgPool2d((sequence_length, 1))\n","    self.gru = nn.GRU(self.embedding_dim, hidden_size, batch_first = True)\n","    self.linear = nn.Linear(hidden_size, 50)\n","\n","  def forward(self, articles, article_lengths):\n","    embeddings = self.embedding(articles)\n","    pooling_outputs = self.pool(embeddings)\n","    gru_inputs = nn.utils.rnn.pack_padded_sequence(pooling_outputs[:, :, 0, :], article_lengths, True, False)\n","    gru_outputs, hidden_state = self.gru(gru_inputs)\n","    predictions = self.linear(hidden_state[0, :, :])\n","    return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PAKC5HKzjMTx"},"source":["from torch import optim\n","\n","second_sequence_length = 25\n","hidden_size = 300\n","learning_rate = 0.1\n","num_epochs = 20\n","batch_size = 10\n","\n","model = ArticleGRU(second_sequence_length, hidden_size, initial_embeddings).cuda()\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWgPLrMLh0LR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621346317864,"user_tz":240,"elapsed":120499,"user":{"displayName":"Jack Schooley","photoUrl":"","userId":"14946235557075291110"}},"outputId":"af2a2380-b97e-4ba5-8f21-be108b1c12b1"},"source":["model.train()\n","batches = int(train_df.shape[0] / batch_size)\n","train_loop_df = train_df.sample(frac = 1, random_state = 2).reset_index()\n","for epoch in range(num_epochs):\n","  print(\"Epoch \" + str(epoch))\n","  for batch in range(batches):\n","    batch_start = batch * batch_size\n","    batch_end = batch_start + batch_size - 1\n","    batch_tensors = train_loop_df.loc[batch_start:batch_end, \"Tensors\"]\n","    batch_lengths = train_loop_df.loc[batch_start:batch_end, \"Sentence Lengths\"]\n","\n","    padded_tensors = [pad_article_tensor(tensor, second_sequence_length) for tensor in batch_tensors]\n","    article_lengths = [len(sentence_lengths) for sentence_lengths in batch_lengths]\n","    articles_tensor = torch.cat([pad_batch(tensor, article_lengths).unsqueeze(0) for tensor in padded_tensors], 0)\n","\n","    inputs = articles_tensor.cuda()\n","    outputs = model(inputs, article_lengths)\n","\n","    batch_authors = train_loop_df.loc[batch_start:batch_end, \"Author\"]\n","    author_tensor = torch.LongTensor([author_dict[author] for author in batch_authors]).cuda()\n","  \n","    loss = loss_fn(outputs, author_tensor)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if batch % 25 == 0:\n","      print(\"Batch \" + str(batch) + \", loss is \" + str(loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0\n","Batch 0, loss is 3.9354217052459717\n","Batch 25, loss is 3.911816120147705\n","Batch 50, loss is 3.7800402641296387\n","Batch 75, loss is 3.904961347579956\n","Batch 100, loss is 3.9620633125305176\n","Batch 125, loss is 3.8098087310791016\n","Batch 150, loss is 3.8419299125671387\n","Batch 175, loss is 3.8408584594726562\n","Batch 200, loss is 3.8589859008789062\n","Batch 225, loss is 3.8566062450408936\n","Batch 250, loss is 3.8191380500793457\n","Batch 275, loss is 3.769514560699463\n","Batch 300, loss is 3.9169399738311768\n","Batch 325, loss is 3.932772159576416\n","Batch 350, loss is 3.8119895458221436\n","Batch 375, loss is 3.8819470405578613\n","Batch 400, loss is 3.683781862258911\n","Batch 425, loss is 3.5658202171325684\n","Epoch 1\n","Batch 0, loss is 3.5090394020080566\n","Batch 25, loss is 3.373082399368286\n","Batch 50, loss is 3.2425904273986816\n","Batch 75, loss is 3.154761552810669\n","Batch 100, loss is 3.9709160327911377\n","Batch 125, loss is 3.1432266235351562\n","Batch 150, loss is 3.629978656768799\n","Batch 175, loss is 3.400782823562622\n","Batch 200, loss is 3.4862544536590576\n","Batch 225, loss is 3.307992935180664\n","Batch 250, loss is 3.2275097370147705\n","Batch 275, loss is 3.095583200454712\n","Batch 300, loss is 3.4744980335235596\n","Batch 325, loss is 3.3553123474121094\n","Batch 350, loss is 3.4376018047332764\n","Batch 375, loss is 3.2306323051452637\n","Batch 400, loss is 2.872852325439453\n","Batch 425, loss is 3.2202179431915283\n","Epoch 2\n","Batch 0, loss is 2.9236412048339844\n","Batch 25, loss is 3.167245388031006\n","Batch 50, loss is 2.847390651702881\n","Batch 75, loss is 2.7182717323303223\n","Batch 100, loss is 3.9466967582702637\n","Batch 125, loss is 2.6729509830474854\n","Batch 150, loss is 3.1579089164733887\n","Batch 175, loss is 2.7717151641845703\n","Batch 200, loss is 3.1843650341033936\n","Batch 225, loss is 2.954561471939087\n","Batch 250, loss is 2.792621612548828\n","Batch 275, loss is 2.585181713104248\n","Batch 300, loss is 3.141838788986206\n","Batch 325, loss is 2.879032850265503\n","Batch 350, loss is 2.8389692306518555\n","Batch 375, loss is 3.0135672092437744\n","Batch 400, loss is 2.1668410301208496\n","Batch 425, loss is 2.8237245082855225\n","Epoch 3\n","Batch 0, loss is 2.9194705486297607\n","Batch 25, loss is 2.8166074752807617\n","Batch 50, loss is 2.436488389968872\n","Batch 75, loss is 2.4206628799438477\n","Batch 100, loss is 3.5482399463653564\n","Batch 125, loss is 2.4256339073181152\n","Batch 150, loss is 2.9583966732025146\n","Batch 175, loss is 2.1905264854431152\n","Batch 200, loss is 2.289165735244751\n","Batch 225, loss is 2.412224769592285\n","Batch 250, loss is 2.3712499141693115\n","Batch 275, loss is 2.144490957260132\n","Batch 300, loss is 2.8002631664276123\n","Batch 325, loss is 2.6016130447387695\n","Batch 350, loss is 2.605806827545166\n","Batch 375, loss is 2.698204755783081\n","Batch 400, loss is 1.7230241298675537\n","Batch 425, loss is 2.576763153076172\n","Epoch 4\n","Batch 0, loss is 2.4726619720458984\n","Batch 25, loss is 2.5659279823303223\n","Batch 50, loss is 2.003153085708618\n","Batch 75, loss is 1.918126106262207\n","Batch 100, loss is 3.2455742359161377\n","Batch 125, loss is 2.0629289150238037\n","Batch 150, loss is 2.372739791870117\n","Batch 175, loss is 1.828725814819336\n","Batch 200, loss is 1.645923376083374\n","Batch 225, loss is 2.161763906478882\n","Batch 250, loss is 1.9233487844467163\n","Batch 275, loss is 1.814112901687622\n","Batch 300, loss is 2.561220645904541\n","Batch 325, loss is 2.3945014476776123\n","Batch 350, loss is 2.4563746452331543\n","Batch 375, loss is 1.8934738636016846\n","Batch 400, loss is 1.614996314048767\n","Batch 425, loss is 2.210249423980713\n","Epoch 5\n","Batch 0, loss is 2.308647394180298\n","Batch 25, loss is 2.3336751461029053\n","Batch 50, loss is 1.5346405506134033\n","Batch 75, loss is 1.5809032917022705\n","Batch 100, loss is 2.855517864227295\n","Batch 125, loss is 1.7746431827545166\n","Batch 150, loss is 2.008328914642334\n","Batch 175, loss is 1.547321081161499\n","Batch 200, loss is 1.666283369064331\n","Batch 225, loss is 2.0356266498565674\n","Batch 250, loss is 1.6854768991470337\n","Batch 275, loss is 1.7435111999511719\n","Batch 300, loss is 2.3965301513671875\n","Batch 325, loss is 2.2264914512634277\n","Batch 350, loss is 2.1611034870147705\n","Batch 375, loss is 1.614729642868042\n","Batch 400, loss is 1.4991662502288818\n","Batch 425, loss is 2.1835975646972656\n","Epoch 6\n","Batch 0, loss is 2.2253775596618652\n","Batch 25, loss is 2.040487289428711\n","Batch 50, loss is 1.28531014919281\n","Batch 75, loss is 1.4512137174606323\n","Batch 100, loss is 2.532514810562134\n","Batch 125, loss is 1.59371018409729\n","Batch 150, loss is 1.8449020385742188\n","Batch 175, loss is 1.3871984481811523\n","Batch 200, loss is 1.3680472373962402\n","Batch 225, loss is 1.992645263671875\n","Batch 250, loss is 1.4541784524917603\n","Batch 275, loss is 1.5584068298339844\n","Batch 300, loss is 2.1619811058044434\n","Batch 325, loss is 2.131685256958008\n","Batch 350, loss is 1.7514073848724365\n","Batch 375, loss is 1.8452173471450806\n","Batch 400, loss is 1.5045511722564697\n","Batch 425, loss is 1.9579851627349854\n","Epoch 7\n","Batch 0, loss is 2.041379451751709\n","Batch 25, loss is 1.9336421489715576\n","Batch 50, loss is 1.1340301036834717\n","Batch 75, loss is 1.3721846342086792\n","Batch 100, loss is 2.379725933074951\n","Batch 125, loss is 1.4660723209381104\n","Batch 150, loss is 1.588780164718628\n","Batch 175, loss is 1.369497537612915\n","Batch 200, loss is 1.2355444431304932\n","Batch 225, loss is 1.8447153568267822\n","Batch 250, loss is 1.3489232063293457\n","Batch 275, loss is 1.4150570631027222\n","Batch 300, loss is 1.983530044555664\n","Batch 325, loss is 1.8553317785263062\n","Batch 350, loss is 1.4605224132537842\n","Batch 375, loss is 1.6226346492767334\n","Batch 400, loss is 1.5151252746582031\n","Batch 425, loss is 1.7038360834121704\n","Epoch 8\n","Batch 0, loss is 1.8417673110961914\n","Batch 25, loss is 1.8789587020874023\n","Batch 50, loss is 1.0253055095672607\n","Batch 75, loss is 1.2894362211227417\n","Batch 100, loss is 2.210869550704956\n","Batch 125, loss is 1.3896325826644897\n","Batch 150, loss is 1.4502309560775757\n","Batch 175, loss is 1.3549515008926392\n","Batch 200, loss is 1.1695177555084229\n","Batch 225, loss is 1.692643165588379\n","Batch 250, loss is 1.2465406656265259\n","Batch 275, loss is 1.4045041799545288\n","Batch 300, loss is 1.9526832103729248\n","Batch 325, loss is 1.591713309288025\n","Batch 350, loss is 1.2861652374267578\n","Batch 375, loss is 1.6471611261367798\n","Batch 400, loss is 1.4854778051376343\n","Batch 425, loss is 1.6192859411239624\n","Epoch 9\n","Batch 0, loss is 1.6971698999404907\n","Batch 25, loss is 1.760443925857544\n","Batch 50, loss is 0.9290108680725098\n","Batch 75, loss is 1.1567224264144897\n","Batch 100, loss is 2.04114031791687\n","Batch 125, loss is 1.3328288793563843\n","Batch 150, loss is 1.5233403444290161\n","Batch 175, loss is 1.3133708238601685\n","Batch 200, loss is 0.9196346402168274\n","Batch 225, loss is 1.5565425157546997\n","Batch 250, loss is 1.1644731760025024\n","Batch 275, loss is 1.3897104263305664\n","Batch 300, loss is 1.8906335830688477\n","Batch 325, loss is 1.4732115268707275\n","Batch 350, loss is 1.2392749786376953\n","Batch 375, loss is 1.2743549346923828\n","Batch 400, loss is 1.1928908824920654\n","Batch 425, loss is 1.4608428478240967\n","Epoch 10\n","Batch 0, loss is 1.626409888267517\n","Batch 25, loss is 1.5741796493530273\n","Batch 50, loss is 0.8881542086601257\n","Batch 75, loss is 1.063889741897583\n","Batch 100, loss is 1.9583126306533813\n","Batch 125, loss is 1.2277030944824219\n","Batch 150, loss is 1.3164188861846924\n","Batch 175, loss is 1.1283535957336426\n","Batch 200, loss is 0.7614153623580933\n","Batch 225, loss is 1.5395729541778564\n","Batch 250, loss is 1.1082518100738525\n","Batch 275, loss is 1.3920950889587402\n","Batch 300, loss is 1.7744371891021729\n","Batch 325, loss is 1.3109185695648193\n","Batch 350, loss is 1.1605807542800903\n","Batch 375, loss is 1.163776159286499\n","Batch 400, loss is 1.1981620788574219\n","Batch 425, loss is 1.2949739694595337\n","Epoch 11\n","Batch 0, loss is 1.5396573543548584\n","Batch 25, loss is 1.3825855255126953\n","Batch 50, loss is 0.8115865588188171\n","Batch 75, loss is 0.9769034385681152\n","Batch 100, loss is 1.8323400020599365\n","Batch 125, loss is 1.1597436666488647\n","Batch 150, loss is 1.1139593124389648\n","Batch 175, loss is 0.9423989057540894\n","Batch 200, loss is 0.6856883764266968\n","Batch 225, loss is 1.5146673917770386\n","Batch 250, loss is 1.0135294198989868\n","Batch 275, loss is 1.423254132270813\n","Batch 300, loss is 1.6489547491073608\n","Batch 325, loss is 1.2268356084823608\n","Batch 350, loss is 1.093946933746338\n","Batch 375, loss is 1.026511788368225\n","Batch 400, loss is 1.177927851676941\n","Batch 425, loss is 1.2618153095245361\n","Epoch 12\n","Batch 0, loss is 1.4905588626861572\n","Batch 25, loss is 1.2395259141921997\n","Batch 50, loss is 0.8490259051322937\n","Batch 75, loss is 0.9513104557991028\n","Batch 100, loss is 1.5272860527038574\n","Batch 125, loss is 1.0093181133270264\n","Batch 150, loss is 1.129995584487915\n","Batch 175, loss is 0.766242504119873\n","Batch 200, loss is 0.7566195130348206\n","Batch 225, loss is 1.4582931995391846\n","Batch 250, loss is 0.9975172877311707\n","Batch 275, loss is 1.4467219114303589\n","Batch 300, loss is 1.6368379592895508\n","Batch 325, loss is 1.15272057056427\n","Batch 350, loss is 0.9823418855667114\n","Batch 375, loss is 0.976445198059082\n","Batch 400, loss is 1.0411016941070557\n","Batch 425, loss is 1.2130415439605713\n","Epoch 13\n","Batch 0, loss is 1.4362980127334595\n","Batch 25, loss is 1.2635797262191772\n","Batch 50, loss is 0.7094066143035889\n","Batch 75, loss is 0.948675274848938\n","Batch 100, loss is 1.5028959512710571\n","Batch 125, loss is 0.9566119909286499\n","Batch 150, loss is 0.7480190992355347\n","Batch 175, loss is 0.818655788898468\n","Batch 200, loss is 0.7101153135299683\n","Batch 225, loss is 1.4115015268325806\n","Batch 250, loss is 0.9418269395828247\n","Batch 275, loss is 1.4625256061553955\n","Batch 300, loss is 1.3943707942962646\n","Batch 325, loss is 1.1310970783233643\n","Batch 350, loss is 0.9316657781600952\n","Batch 375, loss is 0.8030056953430176\n","Batch 400, loss is 0.9147506952285767\n","Batch 425, loss is 1.2888977527618408\n","Epoch 14\n","Batch 0, loss is 1.3153313398361206\n","Batch 25, loss is 1.106415867805481\n","Batch 50, loss is 0.7156744599342346\n","Batch 75, loss is 0.8163028955459595\n","Batch 100, loss is 1.4042795896530151\n","Batch 125, loss is 0.846160888671875\n","Batch 150, loss is 0.625146210193634\n","Batch 175, loss is 0.8270071744918823\n","Batch 200, loss is 0.6453715562820435\n","Batch 225, loss is 1.0702810287475586\n","Batch 250, loss is 0.8771122694015503\n","Batch 275, loss is 1.4796428680419922\n","Batch 300, loss is 1.3720874786376953\n","Batch 325, loss is 1.0618048906326294\n","Batch 350, loss is 0.8782405853271484\n","Batch 375, loss is 0.7911413311958313\n","Batch 400, loss is 0.8546579480171204\n","Batch 425, loss is 1.2064398527145386\n","Epoch 15\n","Batch 0, loss is 1.2761857509613037\n","Batch 25, loss is 1.1209523677825928\n","Batch 50, loss is 0.7378906607627869\n","Batch 75, loss is 0.6269097924232483\n","Batch 100, loss is 1.1496533155441284\n","Batch 125, loss is 0.7630232572555542\n","Batch 150, loss is 0.5375917553901672\n","Batch 175, loss is 0.7891420722007751\n","Batch 200, loss is 0.5263796448707581\n","Batch 225, loss is 0.9229286909103394\n","Batch 250, loss is 0.8161274194717407\n","Batch 275, loss is 1.255557894706726\n","Batch 300, loss is 1.4108830690383911\n","Batch 325, loss is 0.9838666915893555\n","Batch 350, loss is 0.8332351446151733\n","Batch 375, loss is 0.8565080761909485\n","Batch 400, loss is 0.9457351565361023\n","Batch 425, loss is 1.1539459228515625\n","Epoch 16\n","Batch 0, loss is 1.1702582836151123\n","Batch 25, loss is 0.998623251914978\n","Batch 50, loss is 0.639795184135437\n","Batch 75, loss is 0.6319432854652405\n","Batch 100, loss is 1.1653144359588623\n","Batch 125, loss is 0.7290027737617493\n","Batch 150, loss is 0.498321533203125\n","Batch 175, loss is 0.7329758405685425\n","Batch 200, loss is 0.5264496803283691\n","Batch 225, loss is 0.7687238454818726\n","Batch 250, loss is 0.6941055655479431\n","Batch 275, loss is 1.098688006401062\n","Batch 300, loss is 1.3948259353637695\n","Batch 325, loss is 0.9064638018608093\n","Batch 350, loss is 0.7994669675827026\n","Batch 375, loss is 0.9741030931472778\n","Batch 400, loss is 1.0241492986679077\n","Batch 425, loss is 1.2904974222183228\n","Epoch 17\n","Batch 0, loss is 1.1749347448349\n","Batch 25, loss is 1.0489952564239502\n","Batch 50, loss is 0.5638428926467896\n","Batch 75, loss is 0.5265143513679504\n","Batch 100, loss is 1.048926830291748\n","Batch 125, loss is 0.6779314279556274\n","Batch 150, loss is 0.5388344526290894\n","Batch 175, loss is 0.5957106351852417\n","Batch 200, loss is 0.6805109977722168\n","Batch 225, loss is 0.7499593496322632\n","Batch 250, loss is 0.9387852549552917\n","Batch 275, loss is 1.1406474113464355\n","Batch 300, loss is 1.1075364351272583\n","Batch 325, loss is 0.8128612637519836\n","Batch 350, loss is 0.7552651762962341\n","Batch 375, loss is 0.8212869763374329\n","Batch 400, loss is 0.8567224740982056\n","Batch 425, loss is 0.9990348815917969\n","Epoch 18\n","Batch 0, loss is 1.005231261253357\n","Batch 25, loss is 1.6841157674789429\n","Batch 50, loss is 0.6319616436958313\n","Batch 75, loss is 0.5014971494674683\n","Batch 100, loss is 0.809004008769989\n","Batch 125, loss is 0.6108648777008057\n","Batch 150, loss is 0.4402284622192383\n","Batch 175, loss is 0.45075201988220215\n","Batch 200, loss is 1.1196813583374023\n","Batch 225, loss is 0.6742852926254272\n","Batch 250, loss is 0.7402347326278687\n","Batch 275, loss is 1.0955064296722412\n","Batch 300, loss is 1.0694574117660522\n","Batch 325, loss is 0.7877709865570068\n","Batch 350, loss is 0.694278359413147\n","Batch 375, loss is 0.8025053143501282\n","Batch 400, loss is 0.9062610864639282\n","Batch 425, loss is 0.9184259176254272\n","Epoch 19\n","Batch 0, loss is 1.018283724784851\n","Batch 25, loss is 1.037206768989563\n","Batch 50, loss is 0.575976550579071\n","Batch 75, loss is 0.4430198073387146\n","Batch 100, loss is 0.6440866589546204\n","Batch 125, loss is 0.57280433177948\n","Batch 150, loss is 0.46582943201065063\n","Batch 175, loss is 0.40453141927719116\n","Batch 200, loss is 0.5335991978645325\n","Batch 225, loss is 0.5446229577064514\n","Batch 250, loss is 0.7699079513549805\n","Batch 275, loss is 0.7147393226623535\n","Batch 300, loss is 1.1638107299804688\n","Batch 325, loss is 0.736615002155304\n","Batch 350, loss is 0.5730192065238953\n","Batch 375, loss is 0.7445661425590515\n","Batch 400, loss is 0.7367661595344543\n","Batch 425, loss is 0.8186070322990417\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p4lUt7OvuD-8"},"source":["model.eval()\n","batches = int(test_df.shape[0] / batch_size)\n","final_labels = None\n","final_probs = None\n","for batch in range(batches):\n","  batch_start = batch * batch_size\n","  batch_end = batch_start + batch_size - 1\n","  batch_tensors = test_df.loc[batch_start:batch_end, \"Tensors\"]\n","  batch_lengths = test_df.loc[batch_start:batch_end, \"Sentence Lengths\"]\n","\n","  padded_tensors = [pad_article_tensor(tensor, second_sequence_length) for tensor in batch_tensors]\n","  article_lengths = [len(sentence_lengths) for sentence_lengths in batch_lengths]\n","  articles_tensor = torch.cat([pad_batch(tensor, article_lengths).unsqueeze(0) for tensor in padded_tensors], 0)\n","\n","  inputs = articles_tensor.cuda()\n","  \n","  with torch.no_grad():\n","    outputs = model(inputs, article_lengths)\n","  prob_tensor = nn.functional.softmax(outputs, 1)\n","\n","  batch_authors = test_df.loc[batch_start:batch_end, \"Author\"]\n","  label_tensor = torch.LongTensor([author_dict[author] for author in batch_authors])\n","\n","  if final_labels is None:\n","    final_labels = label_tensor\n","  else:\n","    final_labels = torch.cat([final_labels, label_tensor])\n","  \n","  if final_probs is None:\n","    final_probs = prob_tensor\n","  else:\n","    final_probs = torch.cat([final_probs, prob_tensor])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RBimjwtvihx","executionInfo":{"status":"ok","timestamp":1621346325781,"user_tz":240,"elapsed":190,"user":{"displayName":"Jack Schooley","photoUrl":"","userId":"14946235557075291110"}},"outputId":"6c3442d1-12a6-47a9-e8b6-9b6738553550"},"source":["from sklearn import metrics\n","\n","labels = final_labels.numpy()\n","probs = final_probs.cpu().numpy()\n","preds = np.argmax(probs, 1)\n","\n","metrics.accuracy_score(labels, preds)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.562"]},"metadata":{"tags":[]},"execution_count":38}]}]}