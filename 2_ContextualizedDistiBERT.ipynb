{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_AuthorDiscoveringDistilBERT.ipynb","provenance":[{"file_id":"1D3Bihg-VyQ9-jPQQMk9UjpbdawqQAga3","timestamp":1620831756871}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"20a4e869fd7a4b01936748a3336ef0c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_41ffbb35017144ea8e5baaeeb4377d4c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d04d58383c404b068862c4ff56165c67","IPY_MODEL_e69ddd558cca40fa95b45f64bec7971b"]}},"41ffbb35017144ea8e5baaeeb4377d4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d04d58383c404b068862c4ff56165c67":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_edc9e741630b4afb8cda4bbaba39816c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":411,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":411,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_34052b07a5aa4b6ebd1df8ace55cc992"}},"e69ddd558cca40fa95b45f64bec7971b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2c4770b95e9442b0a2c94a4cf56adc6a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 411/411 [00:00&lt;00:00, 911B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_00a5d92077034cc6b277beb82cf93020"}},"edc9e741630b4afb8cda4bbaba39816c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"34052b07a5aa4b6ebd1df8ace55cc992":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2c4770b95e9442b0a2c94a4cf56adc6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"00a5d92077034cc6b277beb82cf93020":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b047ac90de6f4b3981b34988b119d775":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b8b9acbedaa5452e8c634773ad0c7ef4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_28f8fff4572e4714bf1b5e838a0db67a","IPY_MODEL_7a0a2271d309404689b11c3e002d643d"]}},"b8b9acbedaa5452e8c634773ad0c7ef4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"28f8fff4572e4714bf1b5e838a0db67a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_58abe70f400f40d6942648b568870c6f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7d34aa09b93741e09d6594a65e91316c"}},"7a0a2271d309404689b11c3e002d643d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5ed0422698174db28a2cb28b15903375","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:02&lt;00:00, 89.0kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5a1465f5c2444757b2773673c6787828"}},"58abe70f400f40d6942648b568870c6f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7d34aa09b93741e09d6594a65e91316c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5ed0422698174db28a2cb28b15903375":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5a1465f5c2444757b2773673c6787828":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a6cb86e75a7045f0a81120dcd0d09cd2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_26e1b7f2cde6452b9573982ec5601ea1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2965effc6fca45f0be14ff6236b2dca5","IPY_MODEL_b517971eee9149c590aefb16630c5d26"]}},"26e1b7f2cde6452b9573982ec5601ea1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2965effc6fca45f0be14ff6236b2dca5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_19254336ac9646d7bcf4da9ffdfc6ebf","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":435797,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435797,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dd28071e1b054cb68d439eced6df46ee"}},"b517971eee9149c590aefb16630c5d26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e3823827c0634a8eb6c67e206fa5a0fa","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 436k/436k [00:01&lt;00:00, 289kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5630af2976054049a7d77a6fb090744a"}},"19254336ac9646d7bcf4da9ffdfc6ebf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dd28071e1b054cb68d439eced6df46ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e3823827c0634a8eb6c67e206fa5a0fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5630af2976054049a7d77a6fb090744a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42dd78b5c73c4ba9a90036e1442058fd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bd4b124e0d4e423c8beffdab4398670c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d274153647754e0ab9a1d4c9f83abf3e","IPY_MODEL_d2562d9679214cb9bdbcec43f194d269"]}},"bd4b124e0d4e423c8beffdab4398670c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d274153647754e0ab9a1d4c9f83abf3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ade5c6d2383240808761265bf94af8b4","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":29,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0db5310130af46dca3fe9f7a63ab6626"}},"d2562d9679214cb9bdbcec43f194d269":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a79511657cdd4bb9a7ff10f1f84c0c15","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29.0/29.0 [00:00&lt;00:00, 259B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_791d16ec9cd44edba3b5a998fbd9f2c6"}},"ade5c6d2383240808761265bf94af8b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0db5310130af46dca3fe9f7a63ab6626":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a79511657cdd4bb9a7ff10f1f84c0c15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"791d16ec9cd44edba3b5a998fbd9f2c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"9O1JJkBCTa6k"},"source":["# Author Identification with DistilBERT\n","\n","We implement a text classification algorithm using DistilBERT context token (CLS).\n","\n","Adaptation from this notebook: https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"]},{"cell_type":"code","metadata":{"id":"1PMbnkhw1q4v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621454979890,"user_tz":240,"elapsed":29845,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"89e1c9f8-3fae-470e-fd52-a1352c7d654e"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/gdrive')\n","\n","# Jean\n","# os.chdir('/content/gdrive/My Drive/NLP_Project_Author_identification')\n","\n","# Victor\n","os.chdir('/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wVv9noZMi3PX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621454999365,"user_tz":240,"elapsed":16239,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"71eca6f8-3f1e-4e94-c947-e9a9a1a44a5e"},"source":["%%bash\n","# Logistics: install the transformers package\n","pip -q install transformers\n","pip -q install datasets\n","pip -q install tqdm\n","pip -q install sentencepiece\n","\n","import numpy as np"],"execution_count":2,"outputs":[{"output_type":"stream","text":["bash: line 7: import: command not found\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Rf8wFhqqER7O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621454999366,"user_tz":240,"elapsed":15735,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"2b7769cf-ae57-44a9-fca6-723926ea6990"},"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Your runtime has 27.3 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k1hYPCQHsSQN"},"source":["# DistilBERT"]},{"cell_type":"markdown","metadata":{"id":"3o0AA4qItUKP"},"source":["## Parsing of the files"]},{"cell_type":"code","metadata":{"id":"B5IniCUPsR_0","executionInfo":{"status":"ok","timestamp":1621455002694,"user_tz":240,"elapsed":3325,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}}},"source":["### We could augment the train with 25 examples from each author from the text\n","\n","import glob\n","import pandas as pd\n","\n","create_set = False\n","\n","if create_set:\n","  train_df = pd.DataFrame(columns = [\"Author\", \"Article\"], dtype = str)\n","  for i, filename in enumerate(glob.iglob(\"/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification/C50/C50train/*/*.txt\")):\n","    if i % 100 == 0:\n","      print(i)\n","    author = filename.split(\"/\")[8]\n","    with open(filename) as file:\n","      text = file.readlines()\n","    # text = text.replace(\"\\\\n\\',\", \"\").strip(\"]\").replace(\"\\\\n\", \"\").replace(\"\\'\", \"\")\n","    text = \"\".join(text)\n","    text = text.replace(\"\\n\", \" \")\n","    if i == 1:\n","      text11 = text\n","      print(text)\n","    train_df.loc[i] = [author, text]\n","\n","  test_df = pd.DataFrame(columns = [\"Author\", \"Article\"], dtype = str)\n","  for i, filename in enumerate(glob.iglob(\"/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification/C50/C50test/*/*.txt\")):\n","    if i % 100 == 0:\n","      print(i)\n","    author = filename.split(\"/\")[8]\n","    with open(filename) as file:\n","      text = file.readlines()\n","    # text = text.replace(\"\\\\n\\',\", \"\").strip(\"]\").replace(\"\\\\n\", \"\").replace(\"\\'\", \"\")\n","    text = \"\".join(text)\n","    text = text.replace(\"\\n\", \" \")\n","    if i == 1:\n","      text11 = text\n","      print(text)\n","    test_df.loc[i] = [author, text]\n","\n","  # Next, we re-attribute some examples from the test to the train to get a 75%/25% split\n","\n","  authors = test_df.Author.unique()\n","  test_df_final = pd.DataFrame(columns = [\"Author\", \"Article\"], dtype = str)\n","\n","  for a in authors:\n","    sub_df = test_df[test_df['Author'] == a]\n","    mask = np.full(50, False)\n","    mask[:25] = True\n","    np.random.seed(34)\n","    np.random.shuffle(mask)\n","    train_df = pd.concat([train_df, sub_df.iloc[mask]])\n","    test_df_final = pd.concat([test_df_final, sub_df.iloc[~mask]])\n","  \n","  train_df.to_csv('data/train_DistilBERT.csv',index=False)\n","  test_df_final.to_csv('data/test_DistilBERT.csv',index=False)\n","\n","else:\n","  train_df = pd.read_csv('data/train_DistilBERT.csv')\n","  test_df = pd.read_csv('data/test_DistilBERT.csv')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"x0YSluSwxvMC","colab":{"base_uri":"https://localhost:8080/","height":157},"executionInfo":{"status":"ok","timestamp":1621455002699,"user_tz":240,"elapsed":3327,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"9da2d4dd-ae55-4051-a299-4b5a3da2f06f"},"source":["train_df.iloc[0].Article"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'British life assurance group Scottish Amicable on Thursday announced plans to shed its 170-year old mutual status in preparation for a 1.0 billion pounds ($1.68 billion) flotation on the London stock market in three to five years\\' time. The group, based in Stirling, Scotland, said it is to use capital raised by the two-stage proposals to take advantage of opportunities for growth and increase its share of the life and pensions market. Policyholders are to receive an initial special bonus totalling 75 million pounds after demutualising on May 1 this year, based on the length of time policies have been held. At the top end of the scale, an individual with a 25-year policy maturing this April will receive 1,503 pounds. On flotation another, larger payout - expected to amount to at least 200 million pounds but possibly \"substantially higher\" - will be made. The move to seek a flotation is the second by a British life group, following plans announced last year by Norwich Union. Scottish Amicable had been rumoured to be a takeover candidate but chairman Sandy Stewart said the group intended to hold on to its independence. \"Our plans do not include being gobbled up by anybody,\" Stewart told a news briefing. Industry analysts said today\\'s move would make any takeover \"The policyholders are likely to get more by hanging on in there and having an interest in the future than a straight sale now,\" said one. Scottish amicable said it believed the market was entering a growth phase and that it was well positioned to take advantage of an upturn but needed capital and a more flexible structure to take advantage of the opportunities. The period before flotation is intended to allow it to develop the business and necessary profit-focused corporate culture. Managing Director Roy Nicolson said that there remained value to be realised in the group and that an immediate flotation would have failed to reflect the true value of the business and sold policyholders short. Industry analysts described the proposals as a good compromise between raising some capital in the near term but not selling out too cheap. Under the proposals, the business, staff and operations of the group are to be transferred to a new company, Scottish Amicable Life, a 100 percent owned subsidiary of the newly formed Scottish Amicable Holdings. Nicolson, is to become group chief executive of a new holding company Scottish Amicable Holdings. Paul Bradshaw, deputy managing director, will become chief executive of Scottish Amicable Life. The first stage of the group\\'s plans involves a deal with reinsurance giant Swiss Re and its affiliate Securitas Capital to provide an injection of cash of 395 million poounds. Scottish Amicable\\'s existing with-profit fund will receive an immediate cash contribution of 350 million pounds while Securitas Capital will invest up to 45 million pounds in return for approximately 20 percent of a new business fund being set up to transact all new business leading up to flotation. The plans have to be approved by three-quarters of policyholders who vote and a special general meeting is expected to be held in March. ($1=.5958 Pound) '"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"BzdcDRtEtZEx"},"source":["## Transformers and tokenizer"]},{"cell_type":"code","metadata":{"id":"ECoqQtypixub","colab":{"base_uri":"https://localhost:8080/","height":284,"referenced_widgets":["20a4e869fd7a4b01936748a3336ef0c8","41ffbb35017144ea8e5baaeeb4377d4c","d04d58383c404b068862c4ff56165c67","e69ddd558cca40fa95b45f64bec7971b","edc9e741630b4afb8cda4bbaba39816c","34052b07a5aa4b6ebd1df8ace55cc992","2c4770b95e9442b0a2c94a4cf56adc6a","00a5d92077034cc6b277beb82cf93020","b047ac90de6f4b3981b34988b119d775","b8b9acbedaa5452e8c634773ad0c7ef4","28f8fff4572e4714bf1b5e838a0db67a","7a0a2271d309404689b11c3e002d643d","58abe70f400f40d6942648b568870c6f","7d34aa09b93741e09d6594a65e91316c","5ed0422698174db28a2cb28b15903375","5a1465f5c2444757b2773673c6787828","a6cb86e75a7045f0a81120dcd0d09cd2","26e1b7f2cde6452b9573982ec5601ea1","2965effc6fca45f0be14ff6236b2dca5","b517971eee9149c590aefb16630c5d26","19254336ac9646d7bcf4da9ffdfc6ebf","dd28071e1b054cb68d439eced6df46ee","e3823827c0634a8eb6c67e206fa5a0fa","5630af2976054049a7d77a6fb090744a","42dd78b5c73c4ba9a90036e1442058fd","bd4b124e0d4e423c8beffdab4398670c","d274153647754e0ab9a1d4c9f83abf3e","d2562d9679214cb9bdbcec43f194d269","ade5c6d2383240808761265bf94af8b4","0db5310130af46dca3fe9f7a63ab6626","a79511657cdd4bb9a7ff10f1f84c0c15","791d16ec9cd44edba3b5a998fbd9f2c6"]},"executionInfo":{"status":"ok","timestamp":1621455009947,"user_tz":240,"elapsed":10573,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"db05af05-5548-45f4-e3f8-9dbc5bb7aecf"},"source":["### For now we just truncate the context to only take the 512 first tokens - otherwise too long with the positional embedding\n","\n","import transformers\n","\n","from transformers import DistilBertModel, DistilBertConfig\n","\n","example = True\n","\n","### That could have been a way to increase max_position_embeddings, but I can't figure out how\n","# configuration = DistilBertConfig(max_position_embeddings=1024)\n","\n","# Use a pretrained tokenizer with CLASS.from_pretrained() function\n","# \"cased\" or \"uncased\" is whether or not English and english are the same\n","tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-cased') # config=configuration\n","\n","\n","if example:\n","  context = train_df.iloc[0].Article\n","\n","  print(context)\n","\n","  context_ids = tokenizer.encode(context, truncation=True, max_length=512)\n","  print(context_ids)\n","  print(tokenizer.convert_ids_to_tokens(context_ids))"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20a4e869fd7a4b01936748a3336ef0c8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b047ac90de6f4b3981b34988b119d775","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6cb86e75a7045f0a81120dcd0d09cd2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42dd78b5c73c4ba9a90036e1442058fd","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","British life assurance group Scottish Amicable on Thursday announced plans to shed its 170-year old mutual status in preparation for a 1.0 billion pounds ($1.68 billion) flotation on the London stock market in three to five years' time. The group, based in Stirling, Scotland, said it is to use capital raised by the two-stage proposals to take advantage of opportunities for growth and increase its share of the life and pensions market. Policyholders are to receive an initial special bonus totalling 75 million pounds after demutualising on May 1 this year, based on the length of time policies have been held. At the top end of the scale, an individual with a 25-year policy maturing this April will receive 1,503 pounds. On flotation another, larger payout - expected to amount to at least 200 million pounds but possibly \"substantially higher\" - will be made. The move to seek a flotation is the second by a British life group, following plans announced last year by Norwich Union. Scottish Amicable had been rumoured to be a takeover candidate but chairman Sandy Stewart said the group intended to hold on to its independence. \"Our plans do not include being gobbled up by anybody,\" Stewart told a news briefing. Industry analysts said today's move would make any takeover \"The policyholders are likely to get more by hanging on in there and having an interest in the future than a straight sale now,\" said one. Scottish amicable said it believed the market was entering a growth phase and that it was well positioned to take advantage of an upturn but needed capital and a more flexible structure to take advantage of the opportunities. The period before flotation is intended to allow it to develop the business and necessary profit-focused corporate culture. Managing Director Roy Nicolson said that there remained value to be realised in the group and that an immediate flotation would have failed to reflect the true value of the business and sold policyholders short. Industry analysts described the proposals as a good compromise between raising some capital in the near term but not selling out too cheap. Under the proposals, the business, staff and operations of the group are to be transferred to a new company, Scottish Amicable Life, a 100 percent owned subsidiary of the newly formed Scottish Amicable Holdings. Nicolson, is to become group chief executive of a new holding company Scottish Amicable Holdings. Paul Bradshaw, deputy managing director, will become chief executive of Scottish Amicable Life. The first stage of the group's plans involves a deal with reinsurance giant Swiss Re and its affiliate Securitas Capital to provide an injection of cash of 395 million poounds. Scottish Amicable's existing with-profit fund will receive an immediate cash contribution of 350 million pounds while Securitas Capital will invest up to 45 million pounds in return for approximately 20 percent of a new business fund being set up to transact all new business leading up to flotation. The plans have to be approved by three-quarters of policyholders who vote and a special general meeting is expected to be held in March. ($1=.5958 Pound) \n","[101, 1418, 1297, 23296, 1372, 3250, 7277, 4578, 2165, 1113, 9170, 1717, 2714, 1106, 8478, 1157, 10837, 118, 1214, 1385, 9175, 2781, 1107, 7288, 1111, 170, 122, 119, 121, 3775, 6549, 113, 109, 122, 119, 5599, 3775, 114, 22593, 16339, 2116, 1113, 1103, 1498, 4482, 2319, 1107, 1210, 1106, 1421, 1201, 112, 1159, 119, 1109, 1372, 117, 1359, 1107, 16502, 117, 3030, 117, 1163, 1122, 1110, 1106, 1329, 2364, 2120, 1118, 1103, 1160, 118, 2016, 10698, 1106, 1321, 4316, 1104, 6305, 1111, 3213, 1105, 2773, 1157, 2934, 1104, 1103, 1297, 1105, 12966, 1116, 2319, 119, 7037, 17818, 1132, 1106, 3531, 1126, 3288, 1957, 6992, 1703, 1979, 3453, 1550, 6549, 1170, 26707, 3818, 4746, 7131, 1113, 1318, 122, 1142, 1214, 117, 1359, 1113, 1103, 2251, 1104, 1159, 5502, 1138, 1151, 1316, 119, 1335, 1103, 1499, 1322, 1104, 1103, 3418, 117, 1126, 2510, 1114, 170, 1512, 118, 1214, 2818, 22591, 6660, 1142, 1364, 1209, 3531, 122, 117, 1851, 1495, 6549, 119, 1212, 22593, 16339, 2116, 1330, 117, 2610, 2653, 3554, 118, 2637, 1106, 2971, 1106, 1120, 1655, 2363, 1550, 6549, 1133, 3566, 107, 12613, 2299, 107, 118, 1209, 1129, 1189, 119, 1109, 1815, 1106, 5622, 170, 22593, 16339, 2116, 1110, 1103, 1248, 1118, 170, 1418, 1297, 1372, 117, 1378, 2714, 1717, 1314, 1214, 1118, 12867, 1913, 119, 3250, 7277, 4578, 2165, 1125, 1151, 187, 27226, 1174, 1106, 1129, 170, 17748, 3234, 1133, 3931, 9908, 5272, 1163, 1103, 1372, 3005, 1106, 2080, 1113, 1106, 1157, 4574, 119, 107, 3458, 2714, 1202, 1136, 1511, 1217, 1301, 14437, 1146, 1118, 11183, 117, 107, 5272, 1500, 170, 2371, 4094, 1158, 119, 7358, 22018, 1163, 2052, 112, 188, 1815, 1156, 1294, 1251, 17748, 107, 1109, 2818, 17818, 1132, 2620, 1106, 1243, 1167, 1118, 5205, 1113, 1107, 1175, 1105, 1515, 1126, 2199, 1107, 1103, 2174, 1190, 170, 2632, 4688, 1208, 117, 107, 1163, 1141, 119, 3250, 1821, 4578, 2165, 1163, 1122, 2475, 1103, 2319, 1108, 5273, 170, 3213, 4065, 1105, 1115, 1122, 1108, 1218, 11059, 1106, 1321, 4316, 1104, 1126, 1146, 23238, 1133, 1834, 2364, 1105, 170, 1167, 13156, 2401, 1106, 1321, 4316, 1104, 1103, 6305, 119, 1109, 1669, 1196, 22593, 16339, 2116, 1110, 3005, 1106, 2621, 1122, 1106, 3689, 1103, 1671, 1105, 3238, 5022, 118, 3378, 6214, 2754, 119, 16522, 2524, 5396, 21283, 15590, 1163, 1115, 1175, 1915, 2860, 1106, 1129, 11326, 1107, 1103, 1372, 1105, 1115, 1126, 5670, 22593, 16339, 2116, 1156, 1138, 2604, 1106, 7977, 1103, 2276, 2860, 1104, 1103, 1671, 1105, 1962, 2818, 17818, 1603, 119, 7358, 22018, 1758, 1103, 10698, 1112, 170, 1363, 13018, 1206, 5920, 1199, 2364, 1107, 1103, 1485, 1858, 1133, 1136, 4147, 1149, 1315, 10928, 119, 2831, 1103, 10698, 117, 1103, 1671, 117, 2546, 1105, 2500, 1104, 1103, 1372, 1132, 1106, 1129, 3175, 1106, 170, 1207, 1419, 117, 3250, 7277, 4578, 2165, 2583, 117, 170, 1620, 3029, 2205, 7049, 1104, 1103, 3599, 1824, 3250, 7277, 4578, 2165, 14575, 119, 21283, 15590, 117, 1110, 1106, 1561, 1372, 2705, 3275, 1104, 170, 1207, 2355, 1419, 3250, 7277, 4578, 2165, 14575, 119, 1795, 26608, 117, 5874, 7204, 1900, 117, 1209, 1561, 2705, 3275, 1104, 102]\n","['[CLS]', 'British', 'life', 'assurance', 'group', 'Scottish', 'Am', '##ica', '##ble', 'on', 'Thursday', 'announced', 'plans', 'to', 'shed', 'its', '170', '-', 'year', 'old', 'mutual', 'status', 'in', 'preparation', 'for', 'a', '1', '.', '0', 'billion', 'pounds', '(', '$', '1', '.', '68', 'billion', ')', 'fl', '##ota', '##tion', 'on', 'the', 'London', 'stock', 'market', 'in', 'three', 'to', 'five', 'years', \"'\", 'time', '.', 'The', 'group', ',', 'based', 'in', 'Stirling', ',', 'Scotland', ',', 'said', 'it', 'is', 'to', 'use', 'capital', 'raised', 'by', 'the', 'two', '-', 'stage', 'proposals', 'to', 'take', 'advantage', 'of', 'opportunities', 'for', 'growth', 'and', 'increase', 'its', 'share', 'of', 'the', 'life', 'and', 'pension', '##s', 'market', '.', 'Policy', '##holders', 'are', 'to', 'receive', 'an', 'initial', 'special', 'bonus', 'total', '##ling', '75', 'million', 'pounds', 'after', 'dem', '##ut', '##ual', '##ising', 'on', 'May', '1', 'this', 'year', ',', 'based', 'on', 'the', 'length', 'of', 'time', 'policies', 'have', 'been', 'held', '.', 'At', 'the', 'top', 'end', 'of', 'the', 'scale', ',', 'an', 'individual', 'with', 'a', '25', '-', 'year', 'policy', 'mat', '##uring', 'this', 'April', 'will', 'receive', '1', ',', '50', '##3', 'pounds', '.', 'On', 'fl', '##ota', '##tion', 'another', ',', 'larger', 'pay', '##out', '-', 'expected', 'to', 'amount', 'to', 'at', 'least', '200', 'million', 'pounds', 'but', 'possibly', '\"', 'substantially', 'higher', '\"', '-', 'will', 'be', 'made', '.', 'The', 'move', 'to', 'seek', 'a', 'fl', '##ota', '##tion', 'is', 'the', 'second', 'by', 'a', 'British', 'life', 'group', ',', 'following', 'plans', 'announced', 'last', 'year', 'by', 'Norwich', 'Union', '.', 'Scottish', 'Am', '##ica', '##ble', 'had', 'been', 'r', '##umour', '##ed', 'to', 'be', 'a', 'takeover', 'candidate', 'but', 'chairman', 'Sandy', 'Stewart', 'said', 'the', 'group', 'intended', 'to', 'hold', 'on', 'to', 'its', 'independence', '.', '\"', 'Our', 'plans', 'do', 'not', 'include', 'being', 'go', '##bbled', 'up', 'by', 'anybody', ',', '\"', 'Stewart', 'told', 'a', 'news', 'brief', '##ing', '.', 'Industry', 'analysts', 'said', 'today', \"'\", 's', 'move', 'would', 'make', 'any', 'takeover', '\"', 'The', 'policy', '##holders', 'are', 'likely', 'to', 'get', 'more', 'by', 'hanging', 'on', 'in', 'there', 'and', 'having', 'an', 'interest', 'in', 'the', 'future', 'than', 'a', 'straight', 'sale', 'now', ',', '\"', 'said', 'one', '.', 'Scottish', 'am', '##ica', '##ble', 'said', 'it', 'believed', 'the', 'market', 'was', 'entering', 'a', 'growth', 'phase', 'and', 'that', 'it', 'was', 'well', 'positioned', 'to', 'take', 'advantage', 'of', 'an', 'up', '##turn', 'but', 'needed', 'capital', 'and', 'a', 'more', 'flexible', 'structure', 'to', 'take', 'advantage', 'of', 'the', 'opportunities', '.', 'The', 'period', 'before', 'fl', '##ota', '##tion', 'is', 'intended', 'to', 'allow', 'it', 'to', 'develop', 'the', 'business', 'and', 'necessary', 'profit', '-', 'focused', 'corporate', 'culture', '.', 'Managing', 'Director', 'Roy', 'Nico', '##lson', 'said', 'that', 'there', 'remained', 'value', 'to', 'be', 'realised', 'in', 'the', 'group', 'and', 'that', 'an', 'immediate', 'fl', '##ota', '##tion', 'would', 'have', 'failed', 'to', 'reflect', 'the', 'true', 'value', 'of', 'the', 'business', 'and', 'sold', 'policy', '##holders', 'short', '.', 'Industry', 'analysts', 'described', 'the', 'proposals', 'as', 'a', 'good', 'compromise', 'between', 'raising', 'some', 'capital', 'in', 'the', 'near', 'term', 'but', 'not', 'selling', 'out', 'too', 'cheap', '.', 'Under', 'the', 'proposals', ',', 'the', 'business', ',', 'staff', 'and', 'operations', 'of', 'the', 'group', 'are', 'to', 'be', 'transferred', 'to', 'a', 'new', 'company', ',', 'Scottish', 'Am', '##ica', '##ble', 'Life', ',', 'a', '100', 'percent', 'owned', 'subsidiary', 'of', 'the', 'newly', 'formed', 'Scottish', 'Am', '##ica', '##ble', 'Holdings', '.', 'Nico', '##lson', ',', 'is', 'to', 'become', 'group', 'chief', 'executive', 'of', 'a', 'new', 'holding', 'company', 'Scottish', 'Am', '##ica', '##ble', 'Holdings', '.', 'Paul', 'Bradshaw', ',', 'deputy', 'managing', 'director', ',', 'will', 'become', 'chief', 'executive', 'of', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2WxQAoGl92at","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621455010130,"user_tz":240,"elapsed":10754,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"3a21d106-6cc5-4ec6-9530-a5922c1ea19b"},"source":["# Create index for the author, as well as dictionary to go from index to author\n","idx2author = list(train_df.Author.sort_values().unique()) # we sort values so that it's easy to re-find the order just in case\n","author2idx = {idx2author[i]: i for i in range(len(idx2author))}\n","n_authors = len(idx2author)\n","print(f\"We have {n_authors} authors.\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["We have 50 authors.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AxAzf8HSBx0I"},"source":["Now, let's process everything with the encoder."]},{"cell_type":"code","metadata":{"id":"nqHcvZJMBvP0","executionInfo":{"status":"ok","timestamp":1621455012495,"user_tz":240,"elapsed":13117,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}}},"source":["import json\n","import random\n","from multiprocessing import Pool\n","from tqdm import tqdm, trange\n","\n","create_set = False\n","\n","def proc_line_init(tokenizer_for_authors):\n","    global tokenizer\n","    tokenizer = tokenizer_for_authors\n","\n","\n","# Preprocess one C50 data point\n","def proc_line(row, truncation = True, max_length = 512):\n","    article = getattr(row, 'Article')\n","    author = row['Author']\n","\n","    article_ids = tokenizer.encode(article, verbose=False, truncation = truncation, max_length = max_length)\n","\n","    author_id = author2idx[author]\n","\n","    samp = {\n","        \"Article\": article,\n","        \"Author\": author,\n","        \"article_ids\": article_ids,\n","        \"author_id\": author_id,\n","    }\n","\n","    return samp\n","\n","\n","# Preprocess corpus\n","def preproc():\n","\n","    article_proc = list()\n","\n","    for index, row in train_df.iterrows():\n","      article_proc.append(proc_line(row))\n","\n","    json.dump(article_proc, open(\"/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification/data/C50_train_enc_DistilBERT.json\", 'w'))\n","\n","    article_proc_test = list()\n","    for index, row in test_df.iterrows():\n","      article_proc_test.append(proc_line(row))\n","\n","    json.dump(article_proc_test, open(\"/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification/data/C50_test_enc_DistilBERT.json\", 'w'))\n","    \n","    return article_proc, article_proc_test\n","\n","if create_set:\n","  article_proc, article_proc_test = preproc()\n","else:\n","  article_proc = json.load(open(\"/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification/data/C50_train_enc_DistilBERT.json\", 'r'))\n","  article_proc_test = json.load(open(\"/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification/data/C50_test_enc_DistilBERT.json\", 'r'))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EeeNhUBZO4c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621455012496,"user_tz":240,"elapsed":13116,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"55186542-08f8-405f-a22f-138def4c640e"},"source":["print(f\"Train has {len(article_proc)} samples.\")\n","print(f\"Test has {len(article_proc_test)} samples.\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Train has 3750 samples.\n","Test has 1250 samples.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vnRZtQI_tsC9"},"source":["## Models"]},{"cell_type":"markdown","metadata":{"id":"SIM3ych7g9Xl"},"source":["We define 2 models:\n","* `AuthorDiscoveringModel0` is the model based on the CLS token.\n","* `AuthorDiscoveringModel1` implements an attention layer on top of the DistilBERT embeddings."]},{"cell_type":"code","metadata":{"id":"tutPNr_vjXbs","executionInfo":{"status":"ok","timestamp":1621455012723,"user_tz":240,"elapsed":13340,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}}},"source":["import torch.nn as nn\n","\n","class ModelOutputs:\n","    def __init__(self, author_logits=None, loss=None):\n","        self.author_logits = author_logits\n","        self.loss = loss\n","\n","class AuthorDiscoveringModel0(nn.Module):\n","\n","    def __init__(self, language_model=None, dropout=0.2):\n","        '''\n","        lm:         a pretrained transformer language model\n","        dropout:    dropout rate for the dropout layer\n","        '''\n","        super(AuthorDiscoveringModel0, self).__init__()\n","\n","        self.author_outputs = nn.Linear(language_model.config.dim, n_authors)\n","        self.language_model = language_model\n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, input_ids=None, attention_mask=None, author=None):\n","        '''\n","        input_ids:          ids of the concatenated input tokens\n","        attention_mask:     concatenated attention masks (ques+ctx)\n","        author:    label (idx) of the author of the articles\n","        '''\n","        \n","        lm_output = self.language_model(\n","            input_ids = input_ids,\n","            attention_mask = attention_mask\n","        )\n","\n","        last_hidden_state = lm_output.last_hidden_state # size (batch_size, seq_len, hidden_size)\n","        # cls token stores a sentence-level embedding, can be found at index 0\n","        cls_token= last_hidden_state[:, 0, :] # size (batch_size, hidden_size)\n","        cls_token = self.dropout(cls_token)\n","\n","        author_logits = None\n","\n","        # author_logits.size() should be (batch_size, n_authors)\n","\n","        author_logits = self.author_outputs(cls_token)  # the linear layer converts from size (batch_size, hidden_size) to size (batch_size, n_authors)\n","\n","        total_loss = None\n","\n","        if author is not None:\n","\n","            loss_fct = nn.CrossEntropyLoss()\n","            \n","            total_loss = loss_fct(author_logits, author)\n","        \n","        return ModelOutputs(\n","            author_logits = author_logits,\n","            loss = total_loss\n","            )\n","        "],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"iYF16JQUg6IR","executionInfo":{"status":"ok","timestamp":1621455012723,"user_tz":240,"elapsed":13337,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}}},"source":["class AuthorDiscoveringModel1(nn.Module):\n","\n","    def __init__(self, language_model=None, dropout=0.2, att_dim = None):\n","        '''\n","        lm:         a pretrained transformer language model\n","        dropout:    dropout rate for the dropout layer\n","        '''\n","        super(AuthorDiscoveringModel1, self).__init__()\n","\n","        self.author_outputs = nn.Linear(language_model.config.dim, n_authors)\n","        self.language_model = language_model\n","        self.dropout = nn.Dropout(dropout)\n","\n","        if att_dim == None:\n","          self.attention = nn.Linear(language_model.config.dim, language_model.config.dim)\n","          self.context_vector = nn.Linear(language_model.config.dim, 1, bias=False)\n","        else:\n","          self.attention = nn.Linear(language_model.config.dim, att_dim)\n","          self.context_vector = nn.Linear(att_dim, 1, bias=False)\n","\n","        self.tanh = nn.Tanh()\n","        self.softmax = nn.Softmax(dim = 1)\n","    \n","    def forward(self, input_ids=None, attention_mask=None, author=None):\n","        '''\n","        input_ids:          ids of the concatenated input tokens\n","        attention_mask:     concatenated attention masks (ques+ctx)\n","        author:    label (idx) of the author of the articles\n","        '''\n","        \n","        lm_output = self.language_model(\n","            input_ids = input_ids,\n","            attention_mask = attention_mask\n","        )\n","\n","        last_hidden_state = lm_output.last_hidden_state # size (batch_size, seq_len, hidden_size)\n","        last_hidden_state = self.dropout(last_hidden_state) # size (batch_size, seq_len, hidden_size)\n","\n","        att = self.tanh(self.attention(last_hidden_state)) # size (batch_size, seq_len, att_dim)\n","        att = self.context_vector(att).squeeze(2)\n","        att = self.softmax(att)\n","\n","\n","        hidden_state = torch.bmm(last_hidden_state.permute(0,2,1), att.unsqueeze(2)) # size (batch_size, hidden_size)\n","\n","        author_logits = None\n","\n","        author_logits = self.author_outputs(hidden_state.squeeze(2))  # (batch_size, n_authors)\n","\n","        total_loss = None\n","\n","        if author is not None:\n","\n","            loss_fct = nn.CrossEntropyLoss()\n","            \n","            total_loss = loss_fct(author_logits, author)\n","        \n","        return ModelOutputs(\n","            author_logits = author_logits,\n","            loss = total_loss\n","            )"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IybgKgGZwUta"},"source":["## Training the model"]},{"cell_type":"code","metadata":{"id":"jR3K3ifCwT8g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621457339279,"user_tz":240,"elapsed":1500,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"84ee73ce-a1e1-430f-db38-19e0056ec125"},"source":["# Initialize the model and use GPU - can be changed from model 0 to model 1 here\n","lm_pretrained = transformers.AutoModel.from_pretrained('distilbert-base-cased')\n","model_name = \"model1\"\n","\n","\n","if model_name == \"model0\":\n","  model = AuthorDiscoveringModel0(lm_pretrained)\n","elif model_name == \"model1\":\n","  model = AuthorDiscoveringModel1(lm_pretrained)\n","else:\n","  print(\"Please enter valid model name\")\n","model = model.cuda()"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"qjEdfd8ZwhbS"},"source":["**Then we define the training hyper-parameters, the optimizer, and the learning rate scheduler. Read this [document](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup) to understand how the linear learning rate scheduling influences the learning process.**"]},{"cell_type":"code","metadata":{"id":"bWR9NnBfwa0H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621457343549,"user_tz":240,"elapsed":568,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"0871f8ff-f67f-4d71-f985-27c9165d90f6"},"source":["## TODO WITH MY PARAMETERS\n","\n","import torch\n","\n","# Hyper-parameters: you could try playing with different settings\n","num_epochs = 15 # Default will be 10 epochs\n","learning_rate = 3e-5\n","weight_decay = 1e-5\n","eps = 1e-6\n","batch_size = 32\n","warmup_rate = 0.05\n","art_max_length = 512\n","\n","# Calculating the number of warmup steps\n","num_training_cases = len(article_proc)\n","t_total = (num_training_cases // batch_size + 1) * num_epochs\n","ext_warmup_steps = int(warmup_rate * t_total)\n","\n","# Initializing an AdamW optimizer\n","ext_optim = torch.optim.AdamW(model.parameters(), lr=learning_rate,\n","                              eps=eps, weight_decay=weight_decay)\n","\n","# Initializing the learning rate scheduler [details are in the BERT paper]\n","ext_sche = transformers.get_linear_schedule_with_warmup(\n","    ext_optim, num_warmup_steps=ext_warmup_steps, num_training_steps=t_total\n",")\n","\n","print(\"***** Training Info *****\")\n","print(\"  Num examples = %d\" % t_total)\n","print(\"  Num Epochs = %d\" % num_epochs)\n","print(\"  Batch size = %d\" % batch_size)\n","print(\"  Total optimization steps = %d\" % t_total)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["***** Training Info *****\n","  Num examples = 1770\n","  Num Epochs = 15\n","  Batch size = 32\n","  Total optimization steps = 1770\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"asVxAQa7BcMF","executionInfo":{"status":"ok","timestamp":1621457344724,"user_tz":240,"elapsed":388,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}}},"source":["def gather_batch(batch):\n","    art_batch  = [x['Article'] for x in batch]\n","    author_batch = [x['Author'] for x in batch]\n","    author_id_batch = [x['author_id'] for x in batch]\n","\n","    return art_batch, author_batch, author_id_batch"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"09KCNGMec_UW","executionInfo":{"status":"ok","timestamp":1621457345351,"user_tz":240,"elapsed":480,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}}},"source":["def vectorize_batch(batch, tokenizer):\n","    art_batch, author_batch, author_id_batch = gather_batch(batch)\n","\n","    # Encode the article\n","    art_encode = tokenizer.batch_encode_plus(\n","        art_batch,\n","        max_length = art_max_length,\n","        truncation = True,\n","        padding = 'longest',\n","        return_attention_mask = True,\n","        return_tensors = 'pt'\n","    )\n","\n","    # Move the training batch to GPU\n","    art_ids = art_encode['input_ids'].cuda()\n","    art_attn_mask = art_encode['attention_mask'].cuda()\n","\n","    # Move start and end positions to the GPU\n","    author_id_batch = torch.LongTensor(author_id_batch).cuda()\n","\n","    return art_ids, art_attn_mask, author_id_batch"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3pjt_TkevwE","executionInfo":{"status":"ok","timestamp":1621457350706,"user_tz":240,"elapsed":3658,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}}},"source":["Train = False\n","epoch_to_load = 14 # epoch starts at 0 so 14 means after 15 epochs\n","\n","if Train:\n","  model.train()\n","  max_grad_norm = 1\n","\n","  step_id = 0\n","  for epoch in range(num_epochs):\n","\n","      random.shuffle(article_proc)\n","\n","      for i in range(0, num_training_cases, batch_size):\n","\n","          batch = article_proc[i: i + batch_size]\n","          art_ids, art_attn_mask, author_id_batch = vectorize_batch(batch, tokenizer)\n","\n","          model.zero_grad() # Does the same as ext_optim.zero_grad()\n","          \n","          # Get the model outputs, including logits and losses\n","          # stored as a ModelOutput object\n","          outputs = model(\n","              input_ids = art_ids, \n","              attention_mask = art_attn_mask, \n","              author = author_id_batch\n","          )\n","\n","          # Back-propagate the loss signal and clip the gradients\n","          loss = outputs.loss.mean()\n","          loss.backward()\n","          torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","\n","          # Update neural network parameters and the learning rate\n","          ext_optim.step()\n","          ext_sche.step() # Update learning rate for better convergence\n","\n","\n","          if step_id % 100 == 0:\n","              print(f'At step {step_id}, the extraction loss = {loss}')\n","\n","          step_id += 1\n","\n","      torch.save(model.state_dict(), f\"/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification/models/author_ID_{model_name}_{epoch}epo.pt\")\n","\n","  print('Finished Training')\n","\n","else:\n","  model.load_state_dict(torch.load(f\"/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification/models/author_ID_{model_name}_{epoch_to_load}epo.pt\"))"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sK6kz8mv1J-b"},"source":["## Testing the model"]},{"cell_type":"markdown","metadata":{"id":"v0YYQRzw1QFU"},"source":["We'll simply maximize the logits out of the model."]},{"cell_type":"code","metadata":{"id":"Bm5thUZKgYVg","executionInfo":{"status":"ok","timestamp":1621457354219,"user_tz":240,"elapsed":379,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}}},"source":["def logits_to_author(author_logits):\n","  '''\n","  Input sizes:\n","      author_logits.size() = (batch_size, n_author)\n","  Output sizes:\n","      author_pred.size() = (batch_size,)\n","  '''\n","\n","  author_id = torch.argmax(author_logits, axis=1, keepdims=True)\n","\n","  return author_id\n","\n","def accuracy_metric(author_pred_list, author_gt_list):\n","  return sum([author_pred_list[i] == author_gt_list[i] for i in range(len(author_pred_list))])/len(author_gt_list)"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"x8_QyNMlAHxP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621457388351,"user_tz":240,"elapsed":27233,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"b472c815-1f0e-430f-820f-a1db6ca6f49a"},"source":["model.eval()\n","\n","def eval_model(article_proc_to_use):\n","  # Prepare the testing set for evaluation\n","  num_test_cases = len(article_proc_to_use)\n","\n","  eval_batch_size = 64\n","\n","  # `author_pred_list` stores the predicted authors\n","  # in the same order as the contexts of the dev set\n","  author_pred_list = []\n","  author_gt_list = [x['Author'] for x in article_proc_to_use]\n","  article_text = [x['Article'] for x in article_proc_to_use]\n","  # Might need to join the article if I split by sentence first here.\n","\n","  for i in range(0, num_test_cases, eval_batch_size):\n","      eval_batch = article_proc_to_use[i: i + eval_batch_size]\n","      art_batch_test = [x['Article'] for x in eval_batch]\n","\n","      # Encode the article\n","      art_encode_test = tokenizer.batch_encode_plus(\n","          art_batch_test,\n","          max_length = art_max_length,\n","          truncation = True,\n","          padding = 'longest',\n","          return_attention_mask = True,\n","          return_tensors = 'pt'\n","      )\n","\n","      # Move the testing batch to GPU\n","      art_ids_test = art_encode_test['input_ids'].cuda()\n","      art_attn_mask_test = art_encode_test['attention_mask'].cuda()\n","\n","      with torch.no_grad():\n","          outputs = model(\n","              art_ids_test,\n","              attention_mask = art_attn_mask_test\n","          )\n","\n","      author_logits = outputs.author_logits\n","\n","      author_pred = logits_to_author(author_logits)\n","\n","      # store predicted answers in lists\n","      author_pred_list = author_pred_list + author_pred.squeeze().tolist()\n","\n","  # Translate indexes to author names\n","  author_pred_list = [idx2author[author_pred_list[i]] for i in range(len(author_pred_list))]\n","\n","  return author_pred_list, author_gt_list, accuracy_metric(author_pred_list, author_gt_list), article_text\n","\n","# Print the evaluation results\n","\n","a_pred_train, a_gt_train, accuracy_train, article_text_train = eval_model(article_proc)\n","a_pred_test, a_gt_test, accuracy_test, article_text_test = eval_model(article_proc_test)\n","\n","print(f\"Training accuracy is {accuracy_train}\")\n","print(f\"Testing accuracy is {accuracy_test}\")"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Training accuracy is 1.0\n","Testing accuracy is 0.7752\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-xyLQchQehi","executionInfo":{"status":"ok","timestamp":1621457388585,"user_tz":240,"elapsed":6112,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"be533b15-9dd5-468c-8608-0a1e9a9b3a83"},"source":["model_output = pd.DataFrame({\"author_pred\": a_pred_train + a_pred_test,\n","                             \"author_gt\": a_gt_train + a_gt_test,\n","                             \"train\": 5000*[0],\n","                             \"article_text\": article_text_train + article_text_test})\n","\n","model_output[\"train\"].iloc[:len(a_pred_train)] = 1\n","\n","\n","model_output.to_csv(f\"/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification/Models_outputs/output_author_ID_{model_name}.csv\")"],"execution_count":46,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ZmcXsem6QnYK"},"source":["## Fine-tuning model per epoch"]},{"cell_type":"code","metadata":{"id":"gx-vv14yBHJj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621457860696,"user_tz":240,"elapsed":438960,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"63b518a1-67c2-4ee8-ca53-8b5e5bbfecf7"},"source":["### We load all the epoch models one after the other, we compute the accuracy and we append it to the dataset\n","\n","model_accuracy = pd.DataFrame({\"model\": []\n","                              ,\"epoch\": [] \n","                              ,\"train_accuracy\": []\n","                              ,\"test_accuracy\": []\n","                               })\n","\n","# We need to change the model at 2 points: model_name and in the definition of the model\n","model_name = \"model1\"\n","\n","lm_pretrained = transformers.AutoModel.from_pretrained('distilbert-base-cased')\n","model = AuthorDiscoveringModel1(lm_pretrained)\n","model = model.cuda()\n","\n","for epoch_to_load in range(15):\n","  # Load the corresponding model\n","  model.load_state_dict(torch.load(f\"/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification/models/author_ID_{model_name}_{epoch_to_load}epo.pt\"))\n","\n","  model.eval()\n","\n","  a_pred_train, a_gt_train, train_accuracy, _ = eval_model(article_proc)\n","  a_pred_test, a_gt_test, test_accuracy, _ = eval_model(article_proc_test)\n","\n","  model_accuracy = model_accuracy.append({'model': model_name, \"epoch\": epoch_to_load, \"train_accuracy\": train_accuracy, \"test_accuracy\": test_accuracy}, ignore_index=True)\n","\n","  if epoch_to_load % 3 == 0:\n","    print(epoch_to_load)\n","\n","model_accuracy.to_csv(f\"/content/gdrive/MyDrive/6.864 - NLP/NLP_Project_Author_identification/Results/{model_name}_model_accuracy.csv\")"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["0\n","3\n","6\n","9\n","12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":514},"id":"rTZGCmqDRReC","executionInfo":{"status":"ok","timestamp":1621457861363,"user_tz":240,"elapsed":646,"user":{"displayName":"Victor Jouault","photoUrl":"","userId":"13827537732196030781"}},"outputId":"67838705-71a6-4cca-f999-3b0ebfbef08a"},"source":["model_accuracy"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>epoch</th>\n","      <th>train_accuracy</th>\n","      <th>test_accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>model1</td>\n","      <td>0.0</td>\n","      <td>0.530400</td>\n","      <td>0.4928</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>model1</td>\n","      <td>1.0</td>\n","      <td>0.699200</td>\n","      <td>0.6328</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>model1</td>\n","      <td>2.0</td>\n","      <td>0.783733</td>\n","      <td>0.6744</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>model1</td>\n","      <td>3.0</td>\n","      <td>0.880000</td>\n","      <td>0.7192</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>model1</td>\n","      <td>4.0</td>\n","      <td>0.910400</td>\n","      <td>0.7352</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>model1</td>\n","      <td>5.0</td>\n","      <td>0.954400</td>\n","      <td>0.7496</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>model1</td>\n","      <td>6.0</td>\n","      <td>0.969333</td>\n","      <td>0.7552</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>model1</td>\n","      <td>7.0</td>\n","      <td>0.991733</td>\n","      <td>0.7584</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>model1</td>\n","      <td>8.0</td>\n","      <td>0.996533</td>\n","      <td>0.7632</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>model1</td>\n","      <td>9.0</td>\n","      <td>0.997867</td>\n","      <td>0.7632</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>model1</td>\n","      <td>10.0</td>\n","      <td>0.999467</td>\n","      <td>0.7728</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>model1</td>\n","      <td>11.0</td>\n","      <td>0.999733</td>\n","      <td>0.7736</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>model1</td>\n","      <td>12.0</td>\n","      <td>1.000000</td>\n","      <td>0.7736</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>model1</td>\n","      <td>13.0</td>\n","      <td>1.000000</td>\n","      <td>0.7744</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>model1</td>\n","      <td>14.0</td>\n","      <td>1.000000</td>\n","      <td>0.7752</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     model  epoch  train_accuracy  test_accuracy\n","0   model1    0.0        0.530400         0.4928\n","1   model1    1.0        0.699200         0.6328\n","2   model1    2.0        0.783733         0.6744\n","3   model1    3.0        0.880000         0.7192\n","4   model1    4.0        0.910400         0.7352\n","5   model1    5.0        0.954400         0.7496\n","6   model1    6.0        0.969333         0.7552\n","7   model1    7.0        0.991733         0.7584\n","8   model1    8.0        0.996533         0.7632\n","9   model1    9.0        0.997867         0.7632\n","10  model1   10.0        0.999467         0.7728\n","11  model1   11.0        0.999733         0.7736\n","12  model1   12.0        1.000000         0.7736\n","13  model1   13.0        1.000000         0.7744\n","14  model1   14.0        1.000000         0.7752"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"SEMV3TGHTf1G"},"source":[""],"execution_count":null,"outputs":[]}]}